(1, ['Fairness Certification for Natural Language\nProcessing and Large Language Models\nVincent Freiberger1and Erik Buchmann2\nDept. of Computer Science, Leipzig University, Germany1,2\nCenter for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI)\nDresden/Leipzig, Germany1,2\nfreiberger@cs.uni-leipzig.de1buchmann@informatik.uni-leipzig.de2\nAbstract. Natural Language Processing (NLP) plays an important role\nin our daily lives, particularly due to the enormous progress of Large Lan-\nguage Models (LLM). However, NLP has many fairness-critical use cases,\ne.g., as an expert system in recruitment or as an LLM-based tutor in ed-\nucation.', 'Since NLP is based on human language, potentially harmful\nbiases can diffuse into NLP systems and produce unfair results, discrim-\ninate against minorities or generate legal issues. Hence, it is important\nto develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certifica-\ntion for NLP. In particular, we have reviewed a large body of literature\non algorithmic fairness, and we have conducted semi-structured expert\ninterviews with a wide range of experts from that area. We have system-\natically devised six fairness criteria for NLP, which can be further refined\ninto 18 sub-categories.', 'Our criteria offer a foundation for operationaliz-\ning and testing processes to certify fairness, both from the perspective\nof the auditor and the audited organization. Keywords: Fairness, Certification, NLP\n1 Introduction\nFairness is important for Natural Language Processing (NLP) approaches: NLP\nis used in high-stakes contexts such as healthcare [1]. It is also integrated into\ndaily-use technologies, e.g., voice assistants like Amazon Alexa [2] or AI-based\nchatbots like ChatGPT [3]. A lack of fairness often materializes as allocative or\nrepresentational harm [4] for marginalized groups [5].', 'An example of allocative\nharm is a resume filtering system, that prefers male applicants [6]. Representa-\ntional harm would be a translation app that translates to gender stereotypes [7],\ncultural stereotypes or demeaning language [6, 8, 9]. To avoid harm, a fair NLP\napplication must not only resist gender bias [10–13], but also ableist [14], eth-\nnical [11, 15, 16], age-related [17], religion-related [18] or sexuality bias [11, 19]. Performant NLP models tend to be opaque and complex [20, 21]. Interactions\nwith such sociotechnical systems are typically also complex [22].', 'Hence, it is not\npractically achievable for users or affected individuals to verify fairness. Fairness\ncertification could be embraced to reduce information asymmetries [23].arXiv:2401.01262v2  [cs.CL]  3 Jan 2024'])

(2, ['2 Freiberger et al. The concern of this paper is to develop a broad set of criteria, that can\nbe used by an auditor to assess and certify the fairness of an approach that\nmakes use of NLP, be it a large language model, a recruiting-tool or an AI-\nchatbot for teaching. This is challenging: First, many different definitions of\nfairness exist [24, 25], and some of them are contradictory [26, 27]. Second, it\nis still unclear yet, which fairness criteria are important for NLP approaches\nand how they impact each other.', 'For instance, residual unfairness may remain\nafter bias mitigation has been resolved [28]. Third, efforts towards fair NLP and\ncertifying fairness of related AI approaches exist [29–34]. But there is neither\nan established nor a holistic framework on how fairness could be certified and\nwhat could be audited [35, 36]. To approach certification criteria that can be\napplied in practice, we need to consider the challenges of professionals tasked\nwith auditing the fairness of a system, as well as the challenges of developers\nthat encounter fairness issues when creating NLP systems.', 'Thus, our research\nquestion is as follows:\nWhat criteria are relevant to consider for fairness certification for NLP ap-\nproaches from a practitioner’s point of view? To approach our research question, we strive for broad, qualitative research. We have decided to derive a basic set of auditable fairness criteria from litera-\nture on NLP, AI fairness and AI certification. Based on this set of criteria, we\nhave developed a concept for a semi-structured interview with stakeholders from\nbusiness and research.', 'We have analyzed the interview transcripts in order to\nfind out (a) which measures for ensuring fairness need to be considered for NLP\napproaches, and (b) how they influence each other. We make four contributions:\n–We outline and structure an extensive, up-to-date body of research literature\non NLP fairness, AI fairness and fairness auditing from the last years. –We describe our qualitative research approach, which is based on a series\nof semi-structured interviews with 14 experts from various areas related to\nNLP and algorithmic fairness. –We devise a hierarchical coding scheme for the certification of fairness for\nNLP approaches.', '–We provide an overview of the six main criteria, with 18 sub-criteria on the\nsecond hierarchy level for the auditing of NLP approaches, which we have\ndevised from the interview transcripts. To the best of our knowledge, we are the first to devise a holistic, hierar-\nchical coding scheme for the fairness certification of NLP approaches, which is\nbacked up by literature and expert interviews. Our findings allow to develop a\nfairness certification for a wide range of NLP applications, including large lan-\nguage models and other text-generating AI approaches.', 'Fairness is related to\ntrust [37] and reduces information asymmetries [23]. Thus, our results help to\nestablish a selling point for consumers, particularly marginalized groups. From\na legal perspective, a certification can be a precautionary measure [34]. Paper structure: Section 2 reviews related work and gives a theoretical\nbackground. Section 3 outlines our research method. The Sections 4 and 5 eval-\nuate and discuss our findings. Finally, Section 6 concludes.'])

(3, ['Fairness Certification 3\n2 Related Work\nIn this section, we review existing literature on the Natural Language Processing\n(NLP) workflow and its modes of use. We provide an overview of fairness in\nArtificial Intelligence (AI) approaches on which NLP builds. Finally, we describe\nexisting certification approaches for such AI approaches. 2.1 Natural Language Processing\nNLP makes human language computable [38], and can be distinguished into Nat-\nural Language Understanding (NLU) and Natural Language Generation (NLG) [38]. NLU refers to a machine’s comprehension of human language and “extracting\nvaluable information for downstream tasks” [38].', 'Text summarization [39, 40],\nintend recognition [40], machine translation [40], named entity recognition [41],\nsentiment analysis [42] or text classification [40] are examples for NLU. NLG refers to producing human-understandable text or speech in natural\nlanguages [43]. This is done by predicting the next token in a sequence of words,\nbased on data sources like graphics, video, text, or audio. Prominent examples\nare AI-based Large Language Models (LLMs) like ChatGPT [3]. Modern NLP is based on language models that allow the creation of word\nembeddings [40,44] in the semantic space of a language.', 'Building an NLP model\nstarts with selecting the corpus, a “collection of linguistic data, either compiled\nfrom written texts or transcribed from recorded speech” [41]. Models are typi-\ncally large-scale Deep Learning models, which are pretrained on very large sets\nof text data [40]. Word embeddings resulting from language models can also\nbe utilized as features for downstream NLP models [10]. Evaluation is typically\nperformed on specific benchmark data sets for the field [40]. 2.2 Artificial Intelligence Fairness\nFairness can be defined as the equally performant treatment of all humans by AI,\nwithout discriminating against any individuals or communities [45].', 'The whole\nlife cycle of AI needs to be considered, as fairness problems can occur at all\nstages [46]. We employ the Cross Industry Standard Process for Data Mining\n(CRISP-DM) [47] when referring to the life cycle of AI. We chose a broad and\ngeneralist definition of fairness to encapsulate different conceptualizations of\nfairness brought up by interview partners (cf. Sec. 4). Fairness and its perception\nare context-dependent and communally derived [48,49]. For measuring fairness,\nnumerous metrics have been discussed in literature [24, 27, 50], some of which\neven (partially) contradict each other. Assessing fairness is pursued on different levels.', 'This necessitates differenti-\nating between individual fairness and (sub)group fairness, which in themselves\nhave substantial normative differences [25,51]. Bias [25] is often brought up when\nfairness is discussed, even if it is not necessarily correlated to fairness [52]. Bias\nis a “dynamic and social and not [just] a statistical issue” [46]. Mitigating biases\ncan be handled during preprocessing, inprocessing or postprocessing [53, 54] in'])

(4, ['4 Freiberger et al. a software life cycle. Preprocessing captures the data processing before infer-\nence. Inprocessing targets the inference process itself. Postprocessing addresses\noperations performed after inference. To understand how biases impact fairness, understanding in what ways they\nare harmful to which specific groups is required [4]. Biases can result in discrim-\nination, i.e., in differences in the predictive power of models for an individual\nbased on membership in different protected groups [55,56]. It may occur directly\nby utilizing protected attributes such as gender, race, disability, or sexuality.', 'It can also be indirectly , via correlations to excluded protected attributes [25]. These attributes may overlap for certain social groups, causing multi-dimensional\ndiscrimination like intersectionality [57]. Different social groups use language differently [58]. NLP approaches tend\nto perform worse for marginalized groups because their usage of language is\nunderrepresented [5]. Language also transmits beliefs about social groups and\nimposes labels on them, representing their societal position [4]. Language use\nreflects power relationships and social discrimination [11]. Biases in human lan-\nguage are surfaced in word embeddings [10, 11].', 'This is problematic, because\nword embeddings are used for downstream NLP models, and biases diffuse into\nthese models [11]. There they can cause allocative or representational harm, and\nmust be targeted by debiasing [10,59]. 2.3 Artificial Intelligence Certification\nAccording to ISO/IEC 17000:2020 [60], certification is a “third party attestation\n(· · ·) related to an object of conformity assessment” [60]. The object of confor-\nmity assessment refers to an entity to which specific needs and expectations are\nput into place [60].', 'A certification authority performs a provider-independent\naudit with comprehensive checks to assess conformity with certification criteria,\nstandards, or performance claims [35,61,62]. Third-party auditors are “indepen-\ndent organizations or individuals with no obligation or contractual relationship\nto the audit target” [35]. A certification communicates information, signals qual-\nity to the system user, and assurance raises trust [62, 63]. This is important in\nhigh-stakes domains [64]. It helps providers to show their legitimacy and to\nimprove their services and products [34]. Certifications are defined by content,\nsource, and process elements [63, 64]. Content captures the specific subject of\nassessments in an audit.', 'The source describes the institution providing accredi-\ntation. Process describes how an assessment for certification is conducted. For AI auditing, use-case-specific approaches have been suggested [35]. Com-\nmon auditing standards do not exist [29, 35] yet. This is problematic, as such\nsystems are increasingly deployed in high-stakes domains [29]. Current fairness\naudits [30, 32, 33] focus on quantitative aspects. Such audits evaluate system\noutputs according to mathematical fairness definitions without procedural or\nqualitative assessment, and without considering intersectionality [35]. This does\nnot necessarily result in a fair system.'])

(5, ['Fairness Certification 5\n3 Our Research Method\nBecause fairness certification of NLP approaches [35, 65] is a novel, barely de-\nfined research topic (cf. Sec. 2), we approach our research question with semi-\nstructured expert interviews. Such interviews bring consistency, focus, and struc-\nture to the interview while offering room for improvisation [65,66]. In particular,\nwe follow Myers’ qualitative research methodology [65] and Corbin & Strauss’\napproach to data analysis [66,67].', 'Our research method consist of the steps literature review ,interview guide ,\ninterviewee selection ,conducting andtranscribing the interviews, coding the in-\nterview transcripts, annotation and reflection andrefinement of the interviews. In the following, we explain these 8 steps. Literature Review: This step provides us with all the information nec-\nessary to develop an interview guide and to interpret the interview results. In\nparticular, we need an overview of NLP, AI fairness, and certification (cf. Sec. 2). Interview Guide: On the basis of the literature review, we developed an\ninterview guide with open questions (see Appendix 3).', 'Our guide structures the\ninterview into 8 parts. In the first part, (1) we welcome our interviewees and (2)\nexplain the organization of the interview. Next, (3) we capture the professional\nbackground of the interviewee, and (4) clarify the terminology for the interview. Then (5) we ask for criteria for NLP development and (6) for sustaining NLP\nfairness over time. After that, (7) we leave room for open topics to talk about. Finally, (8) we thank our interviewee and conclude the interview.', 'Interviewee Selection: Our research question calls for practically appli-\ncable research results drawn from the entire life cycle of NLP approaches. To\ninvestigate relevant criteria for fairness certification, we decided to select inter-\nviewees as follows: The interviewees should be working in the private sector on\nNLP in a relevant role like data scientist, consultant, or manager. They should\nhave a minimum of two years of industry experience in NLP and, ideally, expo-\nsure to fairness in AI. To broaden our findings [65,68], we wanted to gain diverse\nexperts in the dimensions of age, gender, and cultural background.', 'To identify matching candidates, we browsed LinkedIn and Xing as well as\nthe authors’ professional networks. Matching candidates received a flyer with\nbasic information on the research project and the interview guide in advance, to\nlet them judge whether they could provide value and reflect on the topic. On this basis, we selected 12 interviewees. In addition, we made an exception\nto include two more interviewees with a background in academia. These inter-\nviewees were involved in algorithmic audits. Both had more than three years of\nexperience in algorithmic fairness.', 'Table 1 gives an overview of the interview\npartners in the order in which they were interviewed. The first column of the\ntable contains the ID of the interview partner. In the following, we will use the\nID to relate a statement to a person. The second and third columns describe the\nrole of the interviewee and the business area of the interviewee’s company. The\nlast column contains the interviewee’s professional expertise. Conducting the Interviews: We conducted one interview in English and\n13 in German, all of them via video conferences. The interviews followed the'])

(6, ['6 Freiberger et al. Table 1.', 'Overview of interview partners\nIDProfession Affiliation Expertise\nI1CTO Regional,\nAI SolutionsNLU, whole life cycle\nI2Data Scientist International,\nIT ConsultingWide variety in NLP, whole\nlife cycle\nI3Data Scientist National,\nIT ServicesWide variety in NLP, whole\nlife cycle\nI4Team Lead NLP National,\nAI and ResearchWide variety in NLP\nI5CTO National, Conversational\nAI SolutionsNLG\nI6Head of NLP National,\nIT ServicesNLG\nI7Data Scientist International,\nITNLU, search query handling\nI8Associate Vice\nPresident AIInternational,\nIT ConsultingWide variety in NLP\nI9Senior Program\nManagerInternational, IT Conversational AI, ASR\nI10Consultant Multinational,\nIT ConsultingNLU\nI11Tech Lead NLP International,\nIT ConsultingWide variety in NLP\nI12Research Scientist National, Research Algorithmic fairness\nI13Co-Founder National,\nIT ServicesNLU, focus: preprocessing\nI14Senior Applied\nScientistMultinational,\nE-CommerceAlgorithmic fairness,\ncertification\ninterview guide, as explained in the second step.', 'During the main section of the\ninterview, we generally encouraged further input on topics verbally and non-\nverbally by active listening, open questions, and reflecting back to the intervie-\nwee [65]. The average duration of all 14 interviews was 51 minutes. Transcribing the Interviews: We transcribed all interviews manually, fol-\nlowing the guidelines of Dresing and Pehl [69]. We adapted the guidelines by\nleaving out timestamps, which were not needed. To ensure correctness, we asked\nthe interviewees to check our transcripts.', 'Coding the Transcriptions: To gain insight from the transcripts, we de-\nveloped a coding scheme in an iterative approach based on open and axial cod-\ning [67]. Open coding allows us to freely name concepts represented in interview\ndata. We use axial coding to understand concepts’ context, cause, and con-\nsequence. We identify concepts for the coding scheme by interviewees naming\nthem explicitly, by abstracting from specific ideas to concepts, or by identifying\nexamples for a concept.'])

(7, ['Fairness Certification 7\nFig. 1. Top-level codes for the fairness certification of NLP approaches\nIn advance of our results in Sec. 4, Figure 1 shows the main criteria identified\nin our coding scheme. On this level, the coding scheme consists of six criteria,\nthat are relevant for NLP certification. The scheme is divided into the auditing\norganization and the audited one. Governance Criteria are relevant for the\ncertification process for both of them, from opposite points of view. The general\ncharacteristics and aims of the certification are explained by Process Criteria .', 'Finally, Data-Related Criteria ,Project Planning Criteria ,Modeling &\nEvaluation Criteria andOperations Criteria specify processes and contents\nof the audited organization. Note that coding is an iterative process, i.e., Figure 1\nillustrates the last version of dynamic refinement. Annotation and Reflection: We annotated 1162 text passages, utilizing\nmemos to capture the central ideas and concepts represented in the passage. After multiple iterations over the scheme, 1095 coded passages contained 587\noverall codes, with 124 belonging to open coding and 473 to axial coding. Refinement: Conducting interviews is a dynamic process.', 'After conduct-\ning and analyzing our first four interviews, we adapted our interview guide. In\nparticular, we worked on our questions to simplify the interview process and to\navoid unnecessary follow-up questions. Furthermore, we learned that our broad\napproach puts a lot of time pressure on the interviewer, which does not allow us\nto ask in-depth questions. Thus, we decided to ask our interviewees about prior\nexperiences and thoughts on fairness certification first. This allows us to skip\ntopics in the subsequent interview, that the interviewees did not feel qualified\nenough to talk about.'])

(8, ['8 Freiberger et al. 4 Interview Findings\nThis section gives an overview of all criteria for NLP certification that we have\nidentified. Figure 2 illustrates our hierarchy of codes and sub-codes. Note that\nwe further subdivided the sub-codes, which we omit here for lack of space. Ap-\npendix B contains a more detailed description of our results. Fig. 2. Mind map of the coding scheme for the fairness certification of NLP approaches\nIn the following, we explain the coding of our six criteria together with their\nsub-criteria captured by sub-codes. The intervieweees who addressed them are\ndenoted in brackets.', 'Criteria that were mentioned by many interviewees tend to\nbe more important for a fairness certification of NLP approaches. Process Criteria refer to the implementation of a fairness certification pro-\ncess. Our interviewees identified three distinguished aspects of process criteria,\nwhich we modeled as sub-codes. The sub-code Fairness Understanding captures properties of fairness concep-\ntion, that should be considered for fairness certification according to our inter-\nviews. Examples include cultural dependence ( I9,I13,I14), use case dependence\n(I1,I3,I5,I6,I7,I8,I12,I13,I14), dynamic ( I13,I14) and non-binary ( I12) prop-\nerties.', 'It also covers the interviewees’ conceptions of what is to be considered\nfair, which is nuanced, and varies between conceptions like conditional statistical\nparity ( I1,I4,I6,I7), counterfactual fairness ( I3,I4,I5,I10,I13) and notions of\ninclusiveness ( I2,I4,I7,I8,I9,I11). The sub-code Certification Market Factors covers how a certification and\nregulation may affect each other ( I1,I3,I4,I6,I14), the market which may\nbuild around certification ( I1) and the adoption of certification dependent on\nalignment with corporate goals ( I1,I10,I11,I13,I14). InDesign of Assessment , we capture properties for an assessment and its\nscope.', 'A requirement is finding a way to create an audit process that is as\nholistic as possible ( I1,I6,I8,I9,I11,I12,I13,I14). A certification shouldn’t\nput small companies at a disadvantage ( I3,I5,I6,I11). Interviewees discuss\nthe approach to certification. They propose, for instance, to certify individuals'])

(9, ['Fairness Certification 9\ninstead of processes ( I5,I6,I8). Building a certification on existing standards\nand best practices is also agreed upon ( I1,I5,I8,I10,I11,I13,I14). Another\nmajor point of discussion was to what extent a certification process should be\nrisk-dependent in its scope ( I1,I2,I9,I14) or in its obligation to be performed\n(I2,I10,I11). Governance Criteria refer to all measures the certified organization should\nundertake to ensure the integrity of its processes and management. Recall that\ngovernance is important both from the perspective of the auditing and the au-\ndited organization. We have identified two sub-codes for governance criteria,\nwhich encompass several aspects.', 'The sub-code Model Reporting & Transparency follows the idea of disclosing\nbiases, basic information, data understanding, model explanations, and evalua-\ntion. For basic information, our interviewees want fields of application ( I4,I13),\nthe underlying fairness definition, and the frameworks the system was built on\n(I4) to be disclosed. Data understanding covers all relevant information about the\ndata the system utilizes and how it is processed ( I4,I12,I13). Model explanations\ncover disclosing the model with its architecture ( I4,I8,I12) and explainability\nof the model ( I2,I3,I4,I6,I8,I12).', 'For evaluation, interviewees bring up that\nreporting should contain what has been tested regarding the robustness of the\nmodel against biases ( I2,I4). Evaluation metrics and their resulting data from\ntesting should be publicly documented ( I4,I8). Reporting data bias ( I4), model\nbias ( I3), and countermeasures against biases that are implemented ( I3,I13)\nshould also be done. As disclosure mechanisms, our interviewees suggest utiliz-\ning Model Cards ( I2,I3,I4,I8,I12,I13,I14), open sourcing the model entirely\n(I8) or giving the users an option to access relevant information to the prediction\nthe model made for them on demand ( I8).', 'The sub-code Organizational Criteria targets both the audited and the au-\nditing organization. Interviewees highlight that the context of the auditing or-\nganization should be reflected because it can impede its ability to make neutral\nand holistic assessments. This involves the region it is located in ( I13), the po-\nlitical system it is integrated into ( I13), its initiator ( I13), its integration in the\neconomic system ( I2,I10), or the lack in diversity of professional backgrounds of\nits auditors ( I6,I9). The auditing organization consolidates best practices ( I2,\nI6,I9,I12).', 'For the audited organization the diversity of the development team\n(I2,I4,I10,I11,I13), internal accountability mechanisms and employee qualifi-\ncation and training should be checked. Internal accountability involves checking\nfor internal assessments regarding fairness that are put in place by the audited\norganization ( I7,I14). Roles should be assigned clear responsibilities regarding\nfairness ( I1,I2,I3,I6,I7,I9,I10,I11,I14). Interviewees mention that account-\nability also needs someone who is responsible for fairness in the organization\nand supervision targeted at fairness ( I3,I5,I6,I7,I11). Employee qualification\n& training are seen by interviewees to provide a reasonable job fit for employees\n(I3,I6).', 'A holistic overview of the process of training and operating an NLP sys-\ntem and what could go wrong regarding fairness ( I6,I8) should also be provided.'])

(10, ['10 Freiberger et al. Moreover, the interviews reveal the importance of raising general awareness of\nbias issues and ensuring general knowledge about biases ( I1,I3,I4,I6,I7,I11). Project Planning Criteria address processes in the planning stage of a\nproduct and its underlying business problem that the certification institution\nshould assess. Their scope ends right before data understanding in the CRISP-\nDM framework. This involves two sub-codes. For sub-code Define & Assess the Planned Application , the interviewees agree\nabout checking definitions of use case ( I4,I8,I9,I12,I13,I14) and stakeholders\n(I6,I7,I8,I11) provided by the audited organization.', 'This entails an assessment\nfor the latter regarding fairness issues or vulnerabilities to fairness risks. This\ninvolves checking that mechanisms are in place to properly understand users and\nstakeholders, as well as the fairness properties of the use case itself. Moreover,\nthe planned solution should fit the use case, also from a fairness perspective\n(I3,I4,I5,I12). The sub-code Fairness Targets in Requirements implies checking\nif a standardized set of requirements has been established to ensure fairness\nregarding the project over its life cycle ( I2,I4,I9).', 'With Data-related Criteria , we refer to all assessment processes regarding\ndata used for training and retraining as well as regarding procedures for handling\nand transforming data. Those procedures are described by six sub-codes. The sub-code Data Assessment comprises an assessment of data quality re-\ngarding fairness by checking representativeness via distribution checks ( I1,I4,\nI8,I9,I13) and by assessing regarding harmful, toxic or incorrect data ( I2,I4,\nI10,I13,I14). Interviewees suggest introducing processes for bias checks ( I1,I2,\nI4,I5,I7,I9,I11) and assessments of the sourcing ( I1,I2,I4,I5,I6,I8,I12,I13)\nand collection ( I3,I12,I14) of data.', 'Sub-code Annotation captures characteris-\ntics of the annotation process to support fairness. Annotating fairness relevant\nattributes ( I1,I2,I11) is suggested by interviewees. Moreover, comprehensive an-\nnotation guidelines ( I4,I6,I7) and inter-annotator agreements ( I4,I7,I10,I13)\nare important considerations. Sub-code Preprocessing focuses on requirements\nfor filtering ( I3,I4,I13), selecting ( I1,I4,I7,I8,I10,I12,I13), anonymizing ( I3,\nI5,I7,I14) or mapping the data robustly ( I2) to ensure fairness. The sub-code\nData for Evaluation entails that suitable, representative data is used ( I2,I9,I11)\nand discusses where to take it from. Interviewees highlight the importance of\nfairness invariance testing ( I2,I3).', 'Sub-code Data for Continuous Improvement\ncomprises a monitoring process during the operation of the model and a feed-\nback loop targeting improvements and counteracting fairness issues. As criteria\nfor monitoring, interviewees mention fairness test sets ( I2,I3,I6,I8,I9), drift\nmonitoring ( I2,I3,I4,I5,I6,I7,I13), as well as a request assessment for under-\nrepresented groups ( I2) as practices to be implemented by the audited company. Feedback loop practices regarding data need to consider how in-use data like user\nbehavior, usage data or prediction confidence ( I5,I9,I10) and user-made correc-\ntions ( I4) are handled to improve or maintain fairness.', 'Finally, the sub-code\nData Storage covers how data should be stored in development and operations\n(I10) and cached in operations ( I1).'])

(11, ['Fairness Certification 11\nTheModeling & Evaluation Criteria include the two sub-codes Modeling\nand Evaluation, which can be explained as follows:\nModeling is about embedding fairness into the model architecture itself. In-\nterviewees mention integrating hard-coded elements into the system to ensure\nfairness ( I4,I5,I6,I14), training a separate fairness classifier for the model out-\nputs ( I5), working with constraints ( I2) or embedding fairness into the model’s\noptimization itself ( I4,I5,I7,I10). Evaluation investigates what tests should be conducted to evaluate a model\nand how to assess the fairness of a trained model.', 'At the center of this intervie-\nwees discuss functional testing criteria which take an outside perspective on the\nsystem’s outputs given some specific inputs. Interview partners came up with six\nconcepts relevant to this which are validation of predictions ( I1,I4,I6), involving\naffected stakeholders in evaluation ( I5), adversarial testing ( I1,I4,I7), data sets\nor benchmarks for testing ( I2,I3,I6,I8,I9,I11,I13), criteria focused on human-\ncomputer interaction ( I2,I7,I11) and ethics criteria ( I4,I11). Another important\nconsideration for evaluation is what metrics should be utilized to test a system’s\nfairness.', 'Interviewees name metrics focused on robustness or generalization as\nan essential element to measure the model’s likelihood of generating unexpected,\nunfair results ( I2,I4,I8). Impact-based metrics depend on the chosen fairness\nparadigm ( I3,I7,I8,I12,I14). Another approach proposed by I10introduces a\nmetric that penalizes the model’s use of sensitive attributes or unethical content\nwhen calculating loss. Operations Criteria center around what should be considered regarding\nfairness when deploying a model and what mechanisms must be implemented to\nmaintain fairness over time. That subsumes three sub-codes.', 'Sub-code Deployment centers around assessments suited to shipping differ-\nent model sizes and pruning models, which affect their fairness behavior ( I2,\nI3,I6). Interviewees define the sub-code Monitoring as a continuous assessment\nprocedure of the audited company. This is to ensure its system’s fairness over\ntime, involving fairness tests with the roll-out of updates ( I1,I2,I3,I7) and\nintervention strategies to counteract adversarial or unintentional, but harmful,\nmisuse ( I9,I10,I13). Finally, with the sub-code Feedback Loop , interviewees aim\nto get the flow of information back from the operational model assessed.', 'This\ninvolves checking for the option of flagging system outputs that are perceived as\nunfair by users and for an option for users to make corrections ( I1,I2,I3,I7). Interviewees consider leveraging the data which is acquired by that to under-\nstand fairness issues ( I1,I2,I3,I7) and, in some cases, trigger retraining ( I1,I3,\nI4). An assessment includes checking if data utilized for retraining the system\nin continuous improvement is subjected to the same assessments, filtering and\npreprocessing steps as the initial data ( I10). Table 2 provides an overview of which of our interviewees addressed which\n(sub-)code.', 'A comparison of this table with Table 1 indicates that we indeed\nobtained a broad range of experts, which are likely to cover the entire range of\nthe fairness certification process for NLP approaches.'])

(12, ['12 Freiberger et al. Table 2.', 'Second-level coding scheme for fairness certification of NLP approaches\nI1I2I3I4I5I6I7I8I9I10I11I12I13I14\nProcess\nCriteriaFairness\nUnderstanding✓✓✓✓✓✓✓✓✓✓✓✓✓✓\nDesign of Assessment ✓✓✓✓✓✓✓✓✓✓✓✓✓\nCertification Market\nFactors✓ ✓\nProject\nPlanning\nCriteriaDefine & Assess the\nPlanned Application✓✓✓✓✓✓✓✓✓✓✓✓\nFairness Targets in\nRequirements✓✓ ✓\nData-related\nCriteriaData Assessment ✓✓✓✓✓✓✓✓✓✓✓✓✓✓\nAnnotation ✓✓✓✓✓✓ ✓✓✓✓\nPreprocessing ✓✓✓✓✓✓✓✓✓✓✓\nData for Evaluation ✓✓ ✓✓✓ ✓✓\nData for Continuous\nImprovement✓✓✓✓✓✓✓✓ ✓\nData Storage ✓ ✓\nModeling &\nEvaluation\nCriteriaModeling ✓✓✓✓✓✓ ✓ ✓\nEvaluation ✓✓✓✓✓✓✓✓✓✓✓✓✓✓\nOperations\nCriteriaDeployment ✓✓ ✓\nMonitoring ✓✓✓✓✓✓✓ ✓\nFeedback Loop ✓✓✓✓✓✓✓✓✓\nGovernance\nCriteriaModel Reporting &\nTransparency✓✓✓✓✓✓✓ ✓✓✓\nOrganizational\nCriteria✓✓✓✓✓✓✓✓✓✓✓✓✓\n5 Discussion\nIn this section, we briefly discuss our results.', 'We were interested to learn the en-\ntire range of criteria that must be considered to establish fairness certification for\nan NLP approach. To this end, we have conducted and coded 14 semi-structured\ninterviews with a wide span of diverse experts from business and research. Thus,\nwe think that our findings are well applicable to certifying fairness in a corpo-\nrate environment. However, we did not cover other sectors, such as public or\nmilitary. Furthermore, we might not have reached theoretical saturation with\n14 interviewees. Moreover, designing and testing a fairness certification process\nitself was beyond our scope.', 'One might wonder if this work on fairness and bias was influenced by bias\nitself. We explicitly tried to exclude the following biases: Selection bias (selecting\ninterview partners on personal preferences), bias in materials (providing docu-'])

(13, ['Fairness Certification 13\nments before the interviews could have influenced the interviewees), verbal/non-\nverbal bias (due to misunderstandings between interviewer and interviewee) and\nbias in data analysis (subjective coding). We observed, that our interviewees focused particularly on criteria for data\nand functional testing of solutions. However, there is a bias in the relevance\nperception of modeling between NLP fairness research and interviewees’ per-\nspectives. Existing research already considers model architectures inhibiting so-\ncial biases, for instance, via regularization [70–72], adversarial training [6,72] or\nadapting the loss function to support fairness [73, 74].', 'Even though approaches\nmentioned in interviews are consistent with the literature, the topic was barely\nmentioned or deemed relatively unimportant in interviews ( I1,I5). Future re-\nsearch may investigate this mismatch. 6 Conclusion\nFairness certification for natural language processing approaches such as large\nlanguage models, AI-based chatbots or healthcare applications is an important\nissue, that is still unresolved. In this paper, we have conducted and analyzed 14\nsemi-structured expert interviews with mostly NLP experts in the industry and\ntwo algorithmic fairness experts in academia.', 'Our interviewees helped us identify\nsix main criteria and 18 criteria on the second hierarchy level of an open coding\nscheme for certifying the fairness of an NLP approach. Those criteria are an\nimportant building block towards operationalizing and testing NLP processes to\ncertify fairness, from the perspective of the auditor as well as from the perspective\nof the audited organization. Our interviewees have raised plenty of open questions for future research. For instance: How should a certification process handle the use case dependence\nof fairness or its non-binary nature and subjectiveness while in a dynamic en-\nvironment?', 'To what extent would it make sense to make such a certification\nmandatory? On which best practices and standards should a certification be\nbuilt? How extensive should it be? Finally, how must a certification process be\nstructured to specifically address large language models? References\n1. A. Wong, J. M. Plasek, S. P. Montecalvo, and L. Zhou, “Natural language process-\ning and its implications for the future of medication safety: A narrative review of\nrecent advances and challenges,” Pharmacotherapy: The Journal of Human Phar-\nmacology and Drug Therapy , vol. 38, no. 8, pp. 822–841, 2018. 2.', 'I. Lopatovska, K. Rink, I. Knight, K. Raines, K. Cosenza, H. Williams, P. Sorsche,\nD. Hirsch, Q. Li, and A. Martinez, “Talk to me: Exploring user interactions with\nthe amazon alexa,” Journal of Librarianship and Information Science , vol. 51,\nno. 4, pp. 984–997, 2019. 3. OpenAI, “ ChatGPT [large language model],” https://chat.openai.com, 2023.'])

(14, ['14 Freiberger et al. 4. S. L. Blodgett, S. Barocas, H. Daum´ e III, and H. Wallach, “Language (technology)\nis power: A critical survey of “bias” in nlp,” in Proceedings of the 58th annual\nmeeting of the association for computational linguistics , 2020, pp. 5454–5476. 5. N. Markl, “Language variation and algorithmic bias: understanding algorithmic\nbias in British English automatic speech recognition,” in 2022 ACM Conference\non Fairness, Accountability, and Transparency , 2022, pp. 521–534. 6. T. Sun, A. Gaut, S. Tang, Y. Huang, M. ElSherief, J. Zhao et al.', ', “Mitigating\ngender bias in natural language processing: Literature review,” in Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics , 2019,\npp. 1630–1640. 7. G. Stanovsky, N. A. Smith, and L. Zettlemoyer, “Evaluating gender bias in machine\ntranslation,” in Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics , 2019, pp. 1679–1684. 8. A. Caliskan, J. J. Bryson, and A. Narayanan, “Semantics derived automatically\nfrom language corpora contain human-like biases,” Science , vol. 356, no. 6334, pp. 183–186, 2017. 9.', 'L. Weidinger, J. Uesato, M. Rauh, C. Griffin, P.-S. Huang, J. Mellor et al. , “Tax-\nonomy of risks posed by language models,” in 2022 ACM Conference on Fairness,\nAccountability, and Transparency , 2022, pp. 214–229. 10. T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai, “Man is to\ncomputer programmer as woman is to homemaker? debiasing word embeddings,”\ninAdvances in neural information processing systems , 2016, pp. 4349–4357. 11.', 'O. Papakyriakopoulos, S. Hegelich, J. C. M. Serrano, and F. Marco, “Bias in word\nembeddings,” in Proceedings of the 2020 conference on fairness, accountability, and\ntransparency , 2020, pp. 446–457. 12. R. Tatman, “Gender and dialect bias in YouTube’s automatic captions,” in Pro-\nceedings of the first ACL workshop on ethics in natural language processing , 2017,\npp. 53–59. 13.', 'A. Ovalle, P. Goyal, J. Dhamala, Z. Jaggers, K.-W. Chang, A. Galstyan, R. Zemel,\nand R. Gupta, ““i’m fully who i am”: Towards centering transgender and non-\nbinary voices to measure biases in open language generation,” in Proceedings of\nthe 2023 ACM Conference on Fairness, Accountability, and Transparency , 2023, p.\n1246–1266. 14. S. Hassan Awadallah, M. Huenerfauth, and C. O. Alm, “Unpacking the interde-\npendent systems of discrimination: Ableist bias in nlp systems through an intersec-\ntional lens,” in Findings of the Association for Computational Linguistics: EMNLP\n2021, 2021, pp. 3116–3123. 15.', 'B. Bridgeman, C. Trapani, and Y. Attali, “Comparison of human and machine\nscoring of essays: Differences by gender, ethnicity, and country,” Applied Measure-\nment in Education , vol. 25, no. 1, pp. 27–40, 2012. 16. S. L. Blodgett and B. O’Connor, “Racial disparity in natural language pro-\ncessing: A case study of social media african-american english,” arXiv preprint\narXiv:1707.00061 , 2017. 17. M. Diaz, I. Johnson, A. Lazar, A. M. Piper, and D. Gergle, “Addressing age-related\nbias in sentiment analysis,” in Proceedings of the 2018 chi conference on human\nfactors in computing systems , 2018, pp. 1–14. 18.', 'T. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakan-\ntan, P. Shyam, G. Sastry, A. Askell et al. , “Language models are few-shot learners,”\nAdvances in neural information processing systems , vol. 33, pp. 1877–1901, 2020.'])

(15, ['Fairness Certification 15\n19. S. Garg, V. Perot, N. Limtiaco, A. Taly, E. H. Chi, and A. Beutel, “Counterfac-\ntual fairness in text classification through robustness,” in Proceedings of the 2019\nAAAI/ACM Conference on AI, Ethics, and Society , 2019, pp. 219–226. 20. M. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and P. Sen, “A survey\non bias and fairness in machine learning,” arXiv preprint arXiv:2010.00711 , 2020. 21. B. Lepri, N. Oliver, E. Letouz´ e, A. Pentland, and P. Vinck, “Fair, transparent,\nand accountable algorithmic decision-making processes,” Philosophy & Technology ,\nvol. 31, no. 4, pp.', '611–627, 2018. 22. A. Chouldechova and A. Roth, “A snapshot of the frontiers of fairness in machine\nlearning,” Communications of the ACM , vol. 63, no. 5, pp. 82–89, 2020. 23. P. Cihon, M. J. Kleinaltenkamp, J. Schuett, and S. D. Baum, “Ai certification: Ad-\nvancing ethical practice by reducing information asymmetries,” IEEE Transactions\non Technology and Society , vol. 2, no. 4, pp. 200–209, 2021. 24. S. Verma and J. Rubin, “Fairness definitions explained,” in 2018 ACM/IEEE In-\nternational Workshop on Software Fairness , 2018, pp. 1–7. 25.', 'N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A survey on\nbias and fairness in machine learning,” ACM Computing Surveys , vol. 54, no. 6,\npp. 1–35, 2021. 26. A. Chouldechova, “Fair prediction with disparate impact: A study of bias in re-\ncidivism prediction instruments,” Big Data , vol. 5, no. 2, pp. 153–163, 2017. 27. M. Defrance and T. De Bie, “Maximal fairness,” in Proceedings of the 2023 ACM\nConference on Fairness, Accountability, and Transparency , 2023, p. 851–880. 28.', 'N. Kallus and A. Zhou, “Residual unfairness in fair machine learning from preju-\ndiced data,” in International Conference on Machine Learning . PMLR, 2018, pp. 2439–2448. 29. I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-\nLoud, D. Theron, and P. Barnes, “Closing the ai accountability gap,” in Proceedings\nof the 2020 conference on fairness, accountability, and transparency , 2020, pp. 33–\n44. 30. P. Adler, C. Falk, S. Friedler, T. Nix, G. Rybeck, C. Scheidegger, B. Smith,\nand S. Venkatasubramanian, “Auditing black-box models for indirect influence,”\nKnowledge and Information Systems , vol.', '54, no. 1, pp. 95–122, 2018. 31. C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness through\nawareness,” in Proceedings of the 3rd innovations in theoretical computer science\nconference , 2012, pp. 214–226. 32. S. Park, S. Kim, and Y. Lim, “Fairness audit of machine learning models with\nconfidential computing,” in Proceedings of the ACM Web Conference 2022 , 2022,\npp. 3488–3499. 33.', 'S. Segal, Y. Adi, B. Pinkas, C. Baum, C. Ganesh, and J. Keshet, “Fairness in the\neyes of the data: Certifying machine-learning models,” in Proceedings of the 2021\nAAAI/ACM Conference on AI, Ethics, and Society , 2021, pp. 926–935. 34. R. N. Landers and T. S. Behrend, “Auditing the ai auditors: A framework for evalu-\nating fairness and bias in high stakes ai predictive models,” American Psychologist ,\n2022. 35. S. Costanza-Chock, I. D. Raji, and J. Buolamwini, “Who audits the auditors?', 'recommendations from a field scan of the algorithmic auditing ecosystem,” in 2022\nACM Conference on Fairness, Accountability, and Transparency , 2022, pp. 1571–\n1583. 36. F. Petersen, D. Mukherjee, Y. Sun, and M. Yurochkin, “Post-processing for indi-\nvidual fairness,” in Advances in Neural Information Processing Systems , vol. 34,\n2021, pp. 25 944–25 955.'])

(16, ['16 Freiberger et al. 37. C. Starke, J. Baleis, B. Keller, and F. Marcinkowski, “Fairness perceptions of\nalgorithmic decision-making: A systematic review of the empirical literature,” Big\nData & Society , vol. 9, no. 2, 2022. 38. Y. Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu, “Natural language process-\ning (nlp) in management research: A literature review,” Journal of Management\nAnalytics , vol. 7, no. 2, pp. 139–172, 2020. 39. E. D. Liddy, “Natural language processing,” Encyclopedia of Library and Informa-\ntion Science , vol. 2126, p. 2140, 2001. 40.', 'D. D. Otter, J. Medina, and J. Kalita, “A survey of the usages of deep learn-\ning for natural language processing,” IEEE Transactions on Neural Networks and\nLearning Systems , vol. 32, no. 2, pp. 604–624, 2021. 41. D. Khurana, A. Koli, K. Khatter, and S. Singh, “Natural language processing: State\nof the art, current trends and challenges,” Multimedia Tools and Applications , pp. 1–32, 2022. 42. J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack, “Sentiment analyzer: Extracting\nsentiments about a given topic using natural language processing techniques,” in\nThird IEEE international conference on data mining , 2003, pp.', '427–434. 43. D. McDonald, “Natural language generation,” Handbook of Natural Language Pro-\ncessing , vol. 2, pp. 121–144, 2010. 44. A. Matthews, I. Grasso, C. Mahoney, Y. Chen, E. Wali, T. Middleton et al. , “Gen-\nder bias in natural language processing across human languages,” in Proceedings of\nthe First Workshop on Trustworthy Natural Language Processing , 2021, pp. 45–54. 45. M. Ashok, R. Madan, A. Joha, and U. Sivarajah, “Ethical framework for arti-\nficial intelligence and digital technologies,” International Journal of Information\nManagement , vol. 62, no. 2, p. 102433, 2022. 46.', 'E. Ntoutsi, P. Fafalios, U. Gadiraju, V. Iosifidis, W. Nejdl, M.-E. Vidal et al. ,\n“Bias in data-driven artificial intelligence systems—an introductory survey,” Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge Discovery , vol. 10, no. 3,\npp. 1–14, 2020. 47. R. Wirth and J. Hipp, “CRISP-DM: Towards a standard process model for data\nmining,” in Proceedings of the 4th international conference on the practical appli-\ncations of knowledge discovery and data mining , vol. 1, 2000, pp. 29–39. 48. M. Skirpan and M. Gorelick, “The authority of “fair” in machine learning,” arXiv\npreprint arXiv:1706.09976 , 2017. 49.', 'A. Schmidt and M. Wiegand, “A survey on hate speech detection using natural\nlanguage processing,” in Proceedings of the fifth international workshop on natural\nlanguage processing for social media , 2017, pp. 1–10. 50. A. Z. Jacobs, S. L. Blodgett, S. Barocas, H. Daum´ e III, and H. Wallach, “The\nmeaning and measurement of bias: Lessons from natural language processing,” in\nProceedings of the 2020 conference on fairness, accountability, and transparency ,\n2020, p. 706. 51.', 'R. Binns, “On the apparent conflict between individual and group fairness,” in\nProceedings of the 2020 conference on fairness, accountability, and transparency ,\n2020, pp. 514–524. 52. L. Cabello, A. K. Jørgensen, and A. Søgaard, “On the independence of association\nbias and empirical fairness in language models,” in Proceedings of the 2023 ACM\nConference on Fairness, Accountability, and Transparency , 2023, p. 370–378. 53. R. Bellamy, K. Dey, M. Hind, S. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Mar-\ntino, S. Mehta, A. Mojsilovic et al.', ', “Ai fairness 360: An extensible toolkit for\ndetecting and mitigating algorithmic bias,” IBM Journal of Research and Devel-\nopment , vol. 63, no. 4/5, pp. 4–1, 2019.'])

(17, ['Fairness Certification 17\n54. S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, S. Choudhary, E. P. Hamil-\nton, and D. Roth, “A comparative study of fairness-enhancing interventions in ma-\nchine learning,” in Proceedings of the conference on fairness, accountability, and\ntransparency , 2019, pp. 329–338. 55. I. Chen, F. D. Johansson, and D. Sontag, “Why is my classifier discriminatory?”\ninAdvances in neural information processing systems , 2018, pp. 3543–3554. 56.', 'F. Calmon, D. Wei, B. Vinzamuri, K. N. Ramamurthy, and K. R. Varshney, “Op-\ntimized pre-processing for discrimination prevention,” in Proceedings of the 31st\ninternational conference on neural information processing systems , 2017, pp. 3995–\n4004. 57. A. Roy, J. Horstmann, and E. Ntoutsi, “Multi-dimensional discrimination in law\nand machine learning - a comparative overview,” in Proceedings of the 2023 ACM\nConference on Fairness, Accountability, and Transparency , 2023, p. 89–100. 58.', 'C. Harris, M. Halevy, A. Howard, A. Bruckman, and D. Yang, “Exploring the role\nof grammar and word choice in bias toward african american english (aae) in hate\nspeech classification,” in 2022 ACM Conference on Fairness, Accountability, and\nTransparency , 2022, pp. 789–798. 59. J. Chen, I. Berlot-Attwell, X. Wang, S. T. Hossain, and F. Rudzicz, “Exploring text\nspecific and blackbox fairness algorithms in multimodal clinical nlp,” in Proceedings\nof the 3rd Clinical Natural Language Processing Workshop , 2020, pp. 301–312. 60. I. 17000:2020, Conformity assessment–Vocabulary and general principles . ISO,\n2020. 61.', 'IEEE, “Ieee standard for software reviews and audits,” IEEE Std , vol. 1028, pp. 1–53, 2008. 62. S. Lins, T. Kromat, J. L¨ obbers, A. Benlian, and A. Sunyaev, “Why don’t you join\nin? a typology of information system certification adopters,” Decision Sciences ,\nvol. 53, no. 3, pp. 452–485, 2022. 63. J. Lansing, A. Benlian, and A. Sunyaev, ““unblackboxing” decision makers’ inter-\npretations of is certifications in the context of cloud service certifications,” Journal\nof the Association for Information Systems , vol. 19, no. 11, pp. 1064–1096, 2018. 64.', 'N. Scharowski, M. Benk, S. J. K¨ uhne, L. Wettstein, and F. Br¨ uhlmann, “Certifica-\ntion labels for trustworthy ai: Insights from an empirical mixed-method study,” in\nFAccT ’23: Proceedings of the 2023 ACM Conference on Fairness, Accountability,\nand Transparency , 2023, pp. 248 – 260. 65. M. D. Myers, Qualitative research in business & management . Sage Publications\nLimited, 2020. 66. J. Corbin and A. Strauss, Basics of qualitative research techniques and procedures\nfor developing grounded theory . Sage publications, 2015. 67. ——, “Grounded theory research: Procedures, canons, and evaluative criteria,”\nQualitative sociology , vol. 13, no. 1, pp.', '3–21, 1990. 68. M. Jakesch, Z. Bu¸ cinca, S. Amershi, and A. Olteanu, “How different groups pri-\noritize ethical values for responsible ai,” in 2022 ACM Conference on Fairness,\nAccountability, and Transparency , 2022, pp. 310–323. 69. T. Dresing and T. Pehl, Praxisbuch interview, transkription & analyse: Anleitungen\nund regelsysteme f¨ ur qualitativ forschende . Dr. Dresing & Pehl GmbH, 2018. 70. T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma, “Fairness-aware classifier with\nprejudice remover regularizer,” in Machine Learning and Knowledge Discovery in\nDatabases . Springer, 2012, pp. 35–50. 71.', 'M. Veale and R. Binns, “Fairer machine learning in the real world: Mitigating\ndiscrimination without collecting sensitive data,” Big Data & Society , vol. 4, no. 2,\np. 2053951717743530, 2017.'])

(18, ['18 Freiberger et al. 72. M. Yurochkin and Y. Sun, “SenSeI: Sensitive set invariance for enforcing individual\nfairness,” arXiv preprint arXiv:2006.14168 , 2020. 73. C. Dwork, N. Immorlica, A. T. Kalai, and M. Leiserson, “Decoupled classifiers for\ngroup-fair and efficient machine learning,” in Conference on fairness, accountability\nand transparency , 2018, pp. 119–133. 74. F. Kamiran, A. Karim, and X. Zhang, “Decision theory for discrimination-aware\nclassification,” in 2012 IEEE 12th International Conference on Data Mining , 2012,\npp. 924–929. 75.', 'S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, “Algorithmic de-\ncision making and the cost of fairness,” in Proceedings of the 23rd acm sigkdd\ninternational conference on knowledge discovery and data mining , 2017, pp. 797–\n806. 76. M. J. Kusner, J. Loftus, C. Russell, and R. Silva, “Counterfactual fairness,” in\nProceedings of the 31st international conference on neural information processing\nsystems , 2017, pp. 4066–4076. 77.', 'M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson,\nE. Spitzer, I. D. Raji, and T. Gebru, “Model cards for model reporting,” in Pro-\nceedings of the conference on fairness, accountability, and transparency , 2019, pp. 220–229. 78. T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi et al. , “Hugging-\nface’s transformers: State-of-the-art natural language processing,” arXiv preprint\narXiv:1910.03771 , 2019. 79. M. Pushkarna, A. Zaldivar, and O. Kjartansson, “Data cards: Purposeful and trans-\nparent dataset documentation for responsible AI,” in 2022 ACM Conference on\nFairness, Accountability, and Transparency , 2022, pp. 1776–1826.', 'A Interview Guide\nTable 3: Interview guide on criteria for fairness certification of NLP approaches\nMin Min\nacc.Goal Contents and exemplary questions Expected\nresults\n7 7 Buffer, wel-\ncomeRecent projects, travel,... relaxed start\n3 10 Sort out or-\nganizational\nmattersObtain consent for recording, outline objective\nof work, outline process of interview, outline\nanonymization and tools usedGDPR, research\ngoal and proce-\ndure clear\n5 15 Understand\nbackground\nof intervie-\nweeIn which step of the AI lifecycle are you primarily\nin contact with NLP systems? What points of contact have you had with fairness\nso far?', 'What incidents or issues have there been with it? What contact points might there have been about\ncertification?Contact points\nto topic, profes-\nsional practice\nof the intervie-\nwee\n3 18 Getting\nstarted with\nthe topicWhat does fairness of AI or NLP mean to you? How\nwould you define fairness of NLP? What are your initial thoughts on the topic of fair-\nness certification of AI? And NLP specifically?Terminology\nclarified, entry\npoint'])

(19, ['Fairness Certification 19\n10 28 Criteria for\nNLP system\ndevelop-\nmentWhich requirements should be made to processes\nin development to ensure fairness? Why? (Business\nunderstanding, data understanding, preprocessing,\nannotation, modeling, explainability, evaluation)\nWhat are the organizational requirements that\nshould be considered for fairness certification of\nNLP? Why? (Team, structure)\nWhat must suppliers of software components adhere\nto in order to maintain fairness of the end product? Do you see any other issues in the NLP systems\ndevelopment phase that have not yet been raised? What are the biggest challenges for NLP providers?', 'Where are the opportunities?Answers to\nquestions for de-\nvelopment crite-\nria; contextual\nunderstanding\n10 38 Criteria for\nNLP system\noperationAre there any aspects regarding deployment that\nshould be considered? How does an AI system need to be monitored af-\nter it goes live to detect fairness issues that arise? Who needs to be involved and how is the monitoring\norganized? How often and how extensively do you think fairness\nproblems should be checked? How might such an\naudit be conducted? What are the key issues that\nneed special attention? What processes should be initiated in case of fair-\nness violations?', 'How can improvements be made? To what extent is the interaction of users with the\nsystem to be controlled? Why? What interactions of the user with the system have\nto be considered? What are opportunities and challenges for NLP\nproviders?Understanding\nhow to maintain\nfairness in an\noperational\nsystem and why\nthis is so, and\nhow this can\nbe captured in\ncriteria\n5 43 Clear out\nopen topicsAre there any other criteria apart from the ones dis-\ncussed so far that you would use for fairness certifi-\ncation in NLP?', 'What else is important to you to say about fairness\ncertification in NLP?Additional\ndimensions\n2 45 Pointing\nout next\nstepsDescribe follow-up. Do you know anyone else who could help me with\nan interview?Further points\nof contact\nB Detailed Interview Findings\nB.1 Process Criteria\nProcess Criteria target everything generally relevant to establishing a relevant fairness\ncertification process. Interviewees describe concepts that can be clustered in the cate-\ngories of fairness understanding, design of assessment, and certification market factors.', 'Fairness Understanding Having a good understanding of the fairness properties\nfor the given application can be seen as an essential foundation for building an effective\ncertification process ( I12,I14). Findings follow the structure of codes identified in open\ncoding, as introduced in the previous chapter. It makes sense to keep perspective on the\nfindings by first introducing subjective fairness understandings of interviewees. Some\ntended to focus on an equal opportunity aspect, others on equal performance or other\nconceptualization. The different perspectives of interviewees on fairness also may mani-\nfest in their suggestions for certification measures.', 'Interviewees one, four, six, and seven\nshare the fairness perspective of Fairness through Awareness ( I1,I4,I6,I7) which aims\nat providing similar outcomes to similar individuals [31]. This similarity establishes fair'])

(20, ['20 Freiberger et al. treatment on a meritocratic baseline ( I14). Hence, factors that are objectively relevant\nto a decision, like skills, should be the baseline for a decision. Conditional statistical\nparity [75] could also be utilized as a suitable fairness conception from the literature. A form of enablement that may account for socioeconomic backgrounds, like migration\n(I14), could also be considered ( I14). That would allow for the reduction of structural\ninequalities like gender discrimination in our society ( I6). The target could be changing\nthese structural inequalities by actively intervening ( I1,I4,I7).', 'This concept could be\ncaptured by statistical parity, where the demographics of groups that receive a positive\n(or negative) classification meet the demographics of the overall population [31]. This\nconception, however, comes with the limitation that a group’s representation in the\noverall population needs to be large enough to be considered. Treatment based on a\nmeritocratic baseline is closely related to the notion of a non-discriminatory system. It avoids an unjustifiable negative impact on affected individuals ( I3,I5) and is how\ninterviewees three, four, five, ten, and 13 view fairness ( I3,I4,I5,I10,I13).', 'It closely\nresembles the literature’s counterfactual fairness definition, which emphasizes decision\ninvariance to a protected attribute [76]. It may depend on the use case in its context\nwhen utilizing a protected attribute to justify negative impact may be acceptable ( I3,\nI5). As mentioned by interviewees two, four, seven, eight, nine, and eleven, inclusive-\nness ties closely into this notion ( I2,I4,I7,I8,I9,I11). It means a system should not be\nrestricted in accessibility and usability, and there should not be a barrier to usage by a\ngroup ( I2,I11).', 'Ideally, the system also represents a diverse spectrum and avoids echo\nchambers ( I4,I7,I8). There can also be trade-offs between fairness dimensions. For\ninstance, striving for inclusiveness can impair fairness regarding counterfactual fair-\nness in a recruitment context ( I1). Interviewees two, four, nine, and eleven emphasize\nequal performance as their subjective fairness understanding ( I2,I4,I9,I11). It focuses\non delivering the same performance to each user independent of group membership. The system can cope, for instance, with the variance of expression without sacrific-\ning performance for certain groups ( I9).', 'It may even be interpreted as an equal user\nexperience ( I9). Literature also employs the mathematical fairness definition of coun-\nterfactual fairness to capture this conception of fairness [76]. For some use cases like\nvoice assistants, this fairness conception can be even business-critical due to user di-\nversity ( I2,I9). This subjective nature makes it more difficult to clearly define fairness\nin a certification process ( I1,I3,I5,I12). Defining fairness before an assessment is an\nimportant factor for operationalizing a fairness certification ( I5,I8,I9,I12,I14).', 'An-\nother challenge in defining fairness for an assessment is rooted in the strong dependence\non context imposed by the use case ( I1,I3,I5,I6,I7,I8,I12,I13,I14). This use case\ndependence is particularly highlighted by interviewees three, twelve, and 14. The use\ncase may, for instance, determine whether procedural or distributional fairness should\nbe the focus ( I14). Required criteria for designing content for an assessment may vastly\ndiffer between fairness conceptions ( I6,I14).', 'Some interviewees see the large variance in\nuse cases as a reason for calibrating an assessment to the specific use case by adapting\nits contents to the fairness situation the application imposes ( I6,I7,I11,I14). It may\nmake sense to employ different fairness definitions depending on the affected stake-\nholders and context for the certification process, as discussed in “Project Planning\nCriteria” for the audited company, and adapt the certification process accordingly ( I5,\nI8). Moreover, a dynamic environment implies changing fairness perceptions over time,\nso fairness definitions for stakeholders may need to be reassessed ( I13,I14).', 'Ethics and\nmoral considerations are closely related to fairness ( I8,I9,I10,I14). A fairness defi-\nnition can be grounded in the underlying definition of ethics ( I8). Interviewee eight\nquotes Joscha Bach to define ethics as a “[r]ational process negotiation with yourself'])

(21, ['Fairness Certification 21\nand others to decide how to resolve a conflict of interest under the condition of shared\nvalue” ( I8). Hence, grounding a fairness definition in ethics could cope with the varying\nfairness conceptions ( I8). An assessment should ideally not result in a binary outcome\nin the form of fair vs. not fair, but it should be more nuanced, describing to what\ndegree which fairness concepts are met ( I12). Otherwise, it is difficult to determine the\npoint when something starts to become unfair ( I8).', 'It is crucial to view fairness as\na societal issue as our fairness perception is culturally rooted in the same way some\nbiases are ( I6,I12). Different values in another cultural context may cause a regional\ndependence of fairness ( I9,I13,I14). The fairness perception in China may not align\nwith the typical European fairness understanding, for instance ( I13). This societal bias\nposes a challenge for assessing and operating models on a global scale ( I13). In practice,\nthe topic of fairness in NLP systems is barely considered by most development teams\n(I1).', 'That partially leads to a lack of understanding and awareness of the problems\nthat must be addressed in a certification process ( I11,I14). Besides a lack of practical\nunderstanding of creating a good fairness certification process, some fundamental in-\nsights from research to ground such a process are still missing ( I14). Fairness should\nnot be viewed as an isolated concept as it is entangled with other variables of Respon-\nsible Artificial Intelligence (RAI) ( I1,I7,I9). Transparency, for instance, has already\nbeen addressed as a substantial part of a fairness certification.', 'Data security, privacy,\nand reliability are also named as influencing factors on fairness ( I1,I7,I9). Improving\nsuch should not affect fairness negatively but rather benefit it, making an integrated\nassessment of fairness with other RAI variables interesting ( I9). Design of Assessment Interviewees also give insight into what they consider a\nsuitable design of an assessment for certifying fairness. That includes general prop-\nerties of the certification process that the certifying institution should consider and\ninformation about the scope of an assessment. The hierarchical open coding scheme\nfor “Design of Assessment” is displayed in Figure 3. Fig. 3.', 'Criteria relevant to “Design of Assessment” hierarchically mapped\nAssessment Properties : Assessment properties describe how an assessment should\nbe approached and what may be considered when creating such an assessment proce-\ndure. However, potential roadblocks need to be considered for an assessment to gain\ntraction. First, the feasibility of such a holistic certification process that does not in-\nduce unfairness among industry players is questioned to a certain extent ( I3,I5,I6,\nI11). Apart from the resource intensity of undergoing such a certification process which'])

(22, ['22 Freiberger et al. makes it inaccessible for small players ( I1,I13), there is the potential issue of only lim-\nited access which can be granted to the auditor by the company ( I12,I13) and the issue\nof complexity of the topic as well as the systems ( I3,I5) which may make it unattain-\nable in the near future ( I14). A certification process cannot be seen as a guarantee\nthat no bias issues will occur ( I5).', 'A challenge to master for the auditor is to provide\nan assessment that is as objective as possible, which is difficult with humans involved\n(I13). The approach may define how effective a certification process turns out to be. Currently, approaches to auditing the fairness of a system tend to be too selective and\nhence do not cover the entire range of problems that need to be addressed ( I14). Having\nan end-to-end approach covering the system holistically may be essential ( I1,I6,I8,\nI9,I11,I12,I13,I14).', 'Also, hard-coded parts of the system should be investigated in an\nassessment as they sometimes can be decisive for the system’s outputs ( I6,I9). It may\nmake sense to focus on an assessment regarding the most pressing fairness issues so\nthat a system can operate without causing anyone considerable harm ( I1,I2,I3,I10). An approach that is partially practiced in the industry employs a human comparison\nto define the threshold for unfairness ( I4). It may be criticized as it does not reflect\nthe target of systems’ being as fair as possible ( I2,I4).', 'Another approach focuses more\non certifying individuals involved in the systems development and operations than on\nprocesses established in an organization or data and reports generated by it ( I5,I6,I8). Interviewee eight describes the concept of quick checks, which is discussed further in the\nchapter “Criteria for Governance”, as such an employee-focused certification process\nmay be more effective ( I8) and may have a longer-lasting lifespan in ensuring fairness\n(I6). Another perspective envisions a modular certification process where different en-\ndeavors, like anonymizing data, are all certified individually ( I5).', 'The process factors\ndefine specific nuances a certification process should consider and what is required for\nimplementation. The assessment process should first differentiate between assessment\nfor Business-to-Business (B2B) and assessment for Business-to-Consumer (B2C) use\ncases ( I8). In B2B use cases, different aspects like competitive fairness are relevant,\nwhich are irrelevant for B2C and the other way around ( I4,I7). An assessment should\nbe built on standards, KPIs, and best practices ( I1,I5,I8,I10,I11,I13,I14). They\nhelp companies align their processes and offer clarity. Existing frameworks should be\nutilized to build a process on ( I1,I8,I12,I13).', 'Currently, only internal certification\nprocesses are established in some companies, which generate Lessons Learned ( I14). The latter eventually translates over time into the development of an independent\nthird-party assessment facilitated by an exchange of discovered working mechanisms\nbetween practitioners and in exchange with policymakers ( I14). Currently, the develop-\nment of processes for auditing fairness is driven by assessing the practical applicability\nof research ( I14). When an independent third-party certification is first introduced, it\nwill also manifest and improve with learnings out of experimentation and practical ap-\nplication ( I1,I13,I14).', 'When standards are set and a consistent process is established,\nthe reasoning for reaching the result of an assessment is to be addressed, as it should\nbe grounded in a factual foundation ( I13). Transfer learning needs to be considered\nseparately here as the provider of the unspecified pretrained baseline model and the\nprovider of the custom model, which is then trained for the specific task, are involved\n(I2,I9,I11). Interviewee nine argues that the certifications of both models should be\nkept distinct so that the custom model is certified independently of the ideally already\ncertified baseline model ( I9).', 'That may entail that only already certified baseline mod-\nels are safe and viable options when pursuing certification for one’s custom model ( I11). Reviewing the baseline model by oneself as a provider of a custom model is barely fea-\nsible ( I11). Some interviewees argue that with training the custom model ( I2,I9) or'])

(23, ['Fairness Certification 23\ndue to the use case dependence of fairness ( I1), there is a responsibility shift regarding\nfairness from the provider of the baseline model to the provider of the custom model. That is because the data for training on the baseline model defines the final behavior\nof the model regarding fairness ( I1,I2,I9). Scope : Interviewee nine poses the critical question of where the scope of a certifica-\ntion process should end ( I9). Nearly all interviewees discuss the scope of a certification\nprocess to a certain extent ( I1,I2,I3,I4,I5,I6,I7,I8,I9,I11,I12,I13,I14).', 'It may make\nsense to start basic first when introducing a certification process for fairness and grow\nit more nuanced and complex over time ( I9,I14). The generalization of the certification\nprocess is a major concern regarding its scope ( I3,I5,I7,I8,I9,I14). A certification\nshould be as general as possible without sacrificing the ability to properly judge a use\ncase ( I3,I14). There is a need to specify criteria depending on which use case is present\n(I3,I5,I8,I11,I12,I13,I14) or the geographical location the model is used ( I9).', 'As an\nexample, a hiring context and a context in the medical domain may be compared ( I14). In the hiring context, it makes sense to introduce criteria assessing the similarity of\nembeddings between genders in the semantic vector space. In the medical domain, the\ndifferences between genders may be valuable information to improve treatment specif-\nically for each gender. Criteria should not favor semantic similarity there, but rather\nfocus on the result, which is the success of the treatment for each gender.', 'Clustering\ninto similar use cases may be problematic as human perception of similarity may not\nalign with what models represent ( I3), and there needs to be more clarity for assign-\ning clusters ( I14). Another challenge when specifying criteria is determining what is\nto be considered relevant ( I3). The scope that should be certified may also vary in its\nthoroughness. That may depend on either the risk invoked by the system regarding\nunfairness ( I1,I2,I6,I9,I14) or on the number of users affected by it ( I3).', 'The risk of\na system is typically rooted in its application and not just in the underlying technical\nsystem and hence may shift with the application context changing ( I8). For a risk-\nbased differentiation between use cases, it may make sense to hierarchically increase\nthe thoroughness and frequency of an assessment with the societal impact a system has\n(I6,I14). For low-risk applications, interviewee 14 suggests that no external assessment\nmay be required, and an internal assessment should be pursued by the company and\ndocumented ( I14).', 'That way, internal product, and process expertise can be leveraged\nin an internal audit to make it more streamlined ( I14). However, an external assessment\nshould be regularly required for high-risk applications ( I14). It may also make sense to\nrequire more transparency with increasing risk ( I14). To assess the risk of an applica-\ntion, systemic issues like gender discrimination should be considered. This may be done\nby looking at the potential severity of impact in isolation and the cumulated effect of\ncombined negative impacts in the context of our societal system ( I14).', 'Ultimately, it\nis a societal consideration how much emphasis is put on certification ( I14). To push\nforward fairness in the industry, some interviewees suggest making a certification at\nleast in some cases obligatory as they fear lacking utilization of certification without an\nobligation ( I2,I10,I11). Particularly, large language models are mentioned to require\na certification process ( I4). Others argue that certification should much rather provide\nguidelines based on which the application is certified without introducing an obligation\n(I3,I5). Some interviewees suggest that it may make sense to certify a more extensive\nscope than NLP fairness ( I8,I9,I14).', 'A certification for ML or even algorithmic fair-\nness, in general, is suggested ( I8,I9), and an extension of the certification process to\nassess other RAI principles is proposed ( I9).'])

(24, ['24 Freiberger et al. Certification Market Factors Market factors describe how a certification process\nwould influence its major stakeholders to act economically and how the market envi-\nronment is shaped. Currently, uncertainty is in the market as there are no established\nstandards or best practices and regulatory uncertainty ( I1,I14). There is a reciprocal\ninfluence between a certification process and potential future regulation regarding sys-\ntems’ fairness. On one side, the prospect of coming regulation in the field incentivizes\ncompanies to prepare for future compliance by already undergoing certification ( I1,I3,\nI14).', 'On the other hand, certification may guide regulation by introducing standards\nand giving direction to industry players ( I4,I6). An important factor for adopting a\ncertification process lies in its alignment with the company’s business goals and viabil-\nity (I3,I11). The company first needs to accept the fairness criteria, which are assessed\nby the certification process ( I9). It also needs to see the potential benefits of a certifi-\ncation process.', 'A fairness assessment process may make the company less vulnerable\nto potential lawsuits ( I1,I7), signal trustworthiness and reliability to the user ( I8,I9),\nand even drive innovations ( I3,I9). Fairness issues become increasingly critical ( I9),\nand certification may be an important selling point for a system ( I1,I10,I11,I14). The\nmore business-critical fairness becomes, the higher the certification adoption rate ( I13). A whole market may develop around such a certification of fairness, as most companies\nwill require external consulting to ensure compliance with established standards and\ncertification requirements ( I1).', 'B.2 Governance Criteria\nGovernance Criteria focus on governance aspects that impact a system’s fairness and\nshould be assessed in a fairness certification process. A certification process may be\nbeneficial as it can augment the company’s internal assessment procedures and provide\nfeedback that is, in the long term, beneficial for the company ( I7). In the context\nof fairness, governance should establish a moral standard and a reciprocal fairness\nawareness, as users and providers should be able to judge fairness appropriately ( I3,\nI4,I8).', 'This may be established by introducing a certification process, as it is currently\nbarely considered in the industry ( I10). However, there may be one constraint regarding\ngovernance if a company is developing systems for customers who will use them for\ntheir products. The solution provider then needs to satisfy the customer’s wishes and\nhence is restricted regarding governance ( I6,I10). The interviews brought to attention\ntwo major concepts that need to be considered for governance: “Model Reporting &\nTransparency” and “Organizational Criteria”.', 'Model Reporting & Transparency Providing model reporting and transparency\nfocuses on reducing information asymmetries between the system’s provider and stake-\nholders by providing information about the application that may be relevant to stake-\nholders, particularly for identifying biases ( I1,I3,I6,I11,I13) or avoiding the occur-\nrence of fairness issues. It can be seen as an accountability mechanism for the system’s\nprovider ( I1,I7,I13). Model reporting may also provide essential information for an au-\ndit regarding fairness ( I4,I13). Criteria for model reporting are hierarchically mapped\nin Figure 4.', 'Transparency may be essential in maintaining a good reputation as a\ncompany ( I7). It may be limited by confidentiality requirements within the company\n(I13). The relevance of good model reporting and transparency may be particularly\nhigh for API services ( I2). That is because the customer using the service or build-\ning on it faces information asymmetry and only accesses the system via an interface'])

(25, ['Fairness Certification 25\nFig. 4. Criteria relevant to “Model Reporting & Transparency” hierarchically mapped\n(I2). The form in which transparency is provided may also vary. The ideal form of\ntransparency in operations would be given with open-sourcing key components of the\nsystem ( I8). That enables stakeholders to improve components of the system as they\ncan not just understand the system but also participate ( I8), which may be bene-\nficial regarding fairness as affected groups actively can contribute. That way, their\nperspective might be better represented in the system’s continuous improvement.', 'A\nlower degree of stakeholder access may be chosen with only information about the sys-\ntem and its inferences provided as another form of transparency in operations. Then\nthere need to be considerations regarding the design of the interface providing such\ninformation as it should not negatively impact the system’s usability ( I8). Users may\nnot all be interested in transparency about the system ( I8). Interviewee eight suggests\nmaking such information immediately available with the click of a button ( I8). An\noften-mentioned implementation of model reporting can be found in Model Cards or\nclosely related conceptions ( I2,I3,I4,I8,I12,I13,I14).', 'The paper introducing model\ncards [77] is mentioned and referred to by interviewees ( I12,I13,I14). A model card\nreports on model details, intended use, factors that may influence the model regarding\nfairness, metrics evaluation data, training data, quantitative analyses, ethical consid-\nerations, and caveats and recommendations [77]. The version of model cards provided\nby Hugging Face [78] is also mentioned in the context of providing transparency ( I2,\nI3).', 'Interviewee eight suggests an extended and adapted concept ( I8) which inspires\nan adapted version extended with some contents mentioned in this chapter and can\nbe found in Table 5 in the Appendix. Introducing a similar concept to model cards\nleads to better-documented models and improved transparency, which are beneficial,\nas previously mentioned, not just for fairness ( I3,I13). Model cards can help by giv-\ning direction to model users or even providers by raising awareness on fairness issues\nand potential risks of an application and starting a reflection process ( I8).', 'The con-\ntent of disclosures in operations or model reporting can be clustered in biases, basic\ninformation, data understanding, model characteristics, and evaluation. Basic informa-\ntion should include information about the intended fields of application ( I4,I13), the\nunderlying fairness definition, and the frameworks the system was built on ( I4). The\nlatter may be important information for an audit, making the process reproducible\n(I4). Moreover, information about the model’s generalizability in the domain should be'])

(26, ['26 Freiberger et al. provided, giving an indication of model quality ( I4). Furthermore, it should be commu-\nnicated by the company based on what fairness definition it built the system and what\nthe latter excludes ( I12). Data understanding covers all relevant information about the\ndata the system utilizes and how it is processed. This is essential as NLP systems tend\nto be trained on vast corpora of sometimes even confidential data, which makes it hard\nto judge for users what fairness issues may potentially arise ( I13).', 'Data understand-\ning involves providing the context of data collection, as there might be incentives for\nunrepresentative data collection ( I12). Moreover, the data source ( I12), the volume of\ndata ( I4) as well as distributions ( I12) should be made transparent. It should become\nclear what has been used as training data and what as evaluation data ( I4). Data split-\nting and sampling should be described, and their representativeness stated ( I4). All\ndata preprocessing steps should be described ( I12) and a sample of the data may be\nprovided ( I13).', 'Transparency may also be important regarding model explanations ( I1,\nI2,I3,I4,I6,I7,I8,I11,I12). It should first be reported which models are utilized for\nthe system ( I4,I12). It may also make sense to go deeper and document how the model\ntrains itself on all layers ( I8). This ties into how a model learns certain things, which\nis addressed by explainability ( I11). Explainability should allow the user to understand\nthe basis of the system’s decision and contextualize it ( I1,I2,I3,I7,I8). It may be\nessential to make explainability information transparent ( I2,I3,I4,I6,I8,I12).', 'The\nusers’ domain expertise in the specific field may help uncover particular fairness issues\n(I6). Explainability enables a plausibility check by a human to see whether unwanted\ndecision drivers may be involved that could indicate fairness issues ( I2,I3,I4). In some\ncases, explainability may also be a business requirement and needed anyway ( I4). How-\never, there may be a tradeoff in the quality of explainability feasible for a model and\nits performance. That is because large and complex models that make operationalizing\nexplainability difficult tend to perform better ( I2,I3,I4).', 'Smaller, interpretable mod-\nels may be a solution for certain tasks ( I4,I6), as discussed in the chapter “Project\nPlanning Criteria”. Multiple explainability tools are utilized for black-box models for\nwhich a minimum standard that needs to be met may be specified in a certification\n(I2). But it should consider the different suitability of certain tools depending on the\nuse case ( I2). Mentioned tools that may be utilized include attention scores over all\nlayers ( I2), integrated gradient score and token influence prediction ( I2,I4), and visu-\nalization ( I3,I8). The latter may be hard to quantify ( I3).', 'Non-quantifiable measures\ninvolve a process of human sensemaking, which induces a bias ( I3). Because of that,\nquantifiable explainability measures should be preferred ( I3). The last consideration\nfor reporting explainability is the comprehensiveness of the communicated information\nto non-expert users ( I8,I12). Just reporting very technical details on the model and its\ninference is probably useless for most users, and the addition of comprehensive expla-\nnations is required ( I8,I12). The steps taken for evaluation should also be reported. It\nshould be reported what has been tested regarding the robustness of the model against\nbiases ( I2,I4).', 'There may be some form of scoring on standardized test sets targeting\nvarious biases and allowing for a benchmarking of the model, which can be reported\n(I2,I4,I13). Evaluation metrics should be publicly documented and reported ( I4,I8). The reporting of biases covers data bias ( I4), model bias ( I3), and countermeasures\nagainst biases that are implemented ( I3,I13). If biases can’t be mitigated, there should\nbe at least a hint to users of the system priming them for the potential occurrence of\nsuch ( I4,I7).', 'Interviewee two also suggests testing on tasks not intended for the system\nbut may give hints regarding bias issues that may also occur for the original task and\nhence should be reported ( I2).'])

(27, ['Fairness Certification 27\nOrganizational Criteria Organizational criteria revolve around general processes\nestablished in an organization, accountability-related aspects, and team- and qualification-\nrelated aspects. The findings of this work include organizational criteria for the audited\nand considerations for the auditing organization. The hierarchical structure of all men-\ntioned concepts is visualized in Figure 5. Fig. 5. “Organizational Criteria” hierarchically mapped\nConsiderations for the auditing organization mainly focus on who is performing the\nassessment. When first establishing a certification process, it may make sense to collab-\norate with independent auditing firms with expertise ( I12).', 'It may also be important to\nconsider the background of the certifying third party as the region it is located in ( I13),\nthe political system it is integrated into ( I13), its initiator ( I13), or its integration in the\neconomic system ( I2,I10) influence it. Interviewee 13 suggests an international union\nlike the OECD may be a good platform to build such a certification process ( I13). Ex-\nperts should be responsible for the certification assessment process and continuously\nconsolidate and update best practices ( I2,I6,I9,I12).', 'Regarding the background of\nthose experts, interviewees suggest involving linguists ( I9), domain experts ( I6), data\nscientists and data engineers ( I6), and ethics or sociology experts ( I6,I9). Criteria for the Audited Organization : Organizational criteria for the audited or-\nganization include employee qualification and training, internal accountability, and\ndiversity of the development team. For employee qualification and training, the job fit\nof the employee is an important concept ( I3,I6).', 'In a step like annotation, there can\nbe the issue that due to cost reasons, typically, employees assigned to annotate are not\non long-term contracts and hence may also lack the motivation to make a conscious\neffort to follow annotation guidelines ( I6). The latter, however, is crucial for fairness\nas it provides the foundation to train the model ( I6). Who is assigned what task may\nindicate how successfully fairness may be implemented ( I6). Hence, it would make sense\nto assess who is assigned what job and whether qualifications are sufficient to avoid'])

(28, ['28 Freiberger et al. imposing fairness risks ( I6). The awareness for fairness needs to be raised, and an un-\nderstanding of bias issues needs to be present in the organization as there tends to be\nlacking expertise regarding fairness in development teams ( I1,I3,I4,I6,I7,I11). That\nneeds to be done for the development team of an application by sensitizing to potential\nbiases and training ( I1,I3,I6,I7). Again, annotators are highlighted as particularly rel-\nevant for such training ( I3,I6). Moreover, customer-centric teams should be trained in\ndetecting and identifying biases ( I11).', 'That may be achieved by establishing practices\nthat make customer-centric teams build empathy with the affected individuals and\nchange their perspective to understand the affected groups’ perceptions ( I11). Further-\nmore, such practices may be essential when first introducing a certification process as\nthey enhance the overall problem understanding, which in turn helps the certification\nprocess to evolve ( I11). The installment and contents of such training might be subject\nto an audit. Moreover, members of the development team should all have a rough holis-\ntic overview ( I6,I8).', 'A possible measure supportive of this would be offering workshops\nthat foster a holistic overview ( I6). It may be checked that every development team\nmember is aware of their fairness impact on the product ( I8). Contextualizing such\nquick checks for different roles that each have a different perspective on the product\nmay be a valuable approach to fostering fairness ( I8). Tech-focused team members may\nbenefit from a quick check on data understanding and mitigation layers ( I6,I8). Anno-\ntators and non-tech-focused team members can be equipped with a system and process\nunderstanding ( I6,I8).', 'That would shift the focus of the certification audit further onto\nthe employees as their workflow is certified, similar to a checklist ( I8). For internal ac-\ncountability, roles need to be assigned clear responsibilities, internal assessment or audit\nprocedures should be established, and supervision and responsibility for fairness must\nbe addressed. Starting with roles, establishing a fairness officer or team dedicated to\nfairness makes sense ( I1,I2,I6,I7,I9,I10,I14).', 'That person or team should be respon-\nsible for the fairness definition in the specific use case ( I1,I9), conducting an internal\nfairness assessment ( I1,I2,I6) and providing advice on fairness to the development\nteam and supporting it, for instance in its decision making ( I1,I7,I9). The main issue\nthat arises is that such an individual or team requires financial resources ( I2,I11,I14). There may also be an external assessor for fairness who reports to the company ( I1).', 'The development team functions as a problem solver for fairness issues reported by one\nof the above assessors ( I1,I3,I11) without being the responsible instance for detecting\nfairness issues ( I1,I6,I11). It may also make sense for certain use cases to implement\ndifferent levels of clearance, for instance, when confronted with sensitive contents in a\nfeedback loop to ensure appropriate and fairness-compliant handling ( I7). For the spe-\ncific project, interviewees view some form of supervision or fairness responsibility as an\nessential concept.', 'Either the data curator ( I3), a separately consulting organizational\ninstance ( I7), or the project or department manager ( I6,I11) should be responsible for\nfairness in the specific project. The initiative for fairness, however, needs to come from\nthe company’s particular commitments regarding fairness ( I11). Some processes, like\nannotation, also need careful supervision and verification by someone accountable ( I4,\nI6). An internal audit or assessment can also be a vital tool for a company to ensure\naccountability regarding fairness ( I7,I14).', 'An assessment may target the impact an\napplication has on its stakeholders as a foundation for fairness considerations ( I7). It\nmay be updated and tracked continuously, and with changing stakeholders, it needs to\nbe reiterated ( I7). Red teaming is suggested as another form of assessment ( I7). It pro-\nvides a holistic system testing for potential issues in a set time interval. The so-called\n“red team” is to find fairness vulnerabilities in the system. That could be seen as an\nexemplary holistic and focused company internal fairness assessment. Such assessments'])

(29, ['Fairness Certification 29\nare typically motivated by ensuring to be regulatory compliant with future legislation\nbesides ensuring fairness ( I14). These assessments or internal audits should take place\ncontinuously ( I7) and should not be conducted by the development team but rather by\na separate organizational instance ( I14). An aspect that may be assessed which may\nvary in its importance for fairness between use cases, is the amount of human oversight\nin operations ( I4).', 'One more dynamic should be considered regarding accountability\nwhich addresses binding claims made by a supplier about his product that shift re-\nsponsibility back to the supplier ( I1). An assessment should also check the diversity\nof the development team ( I2,I11). Due to different perceptions and realities of life, a\ndiverse development team fosters problem-understanding and thus helps identify and\nsolve fairness issues ( I2). The major challenge lies in defining what a diverse team is\n(I2).', 'It is debatable which dimensions like gender, race, or sexual orientation diversity\nare relevant to consider when building a diverse team ( I2). It should ideally represent\nminorities ( I2). Currently, development teams lack the diversity of their users in di-\nmensions like age and gender ( I2,I11). To be, to a certain degree, representative of\nthe user may be a requirement to check ( I2,I11). Diversity may also be supported via\n(sometimes lacking) interdisciplinary collaboration within the development team ( I6,\nI10).', 'Interviewees particularly highlight diversity in the development team for anno-\ntators as they directly impact and potentially bias data ( I4,I10,I13). That, however,\ncomes with the limitation that someone needs to be qualified to label, which may be\nmore or less restrictive, dependent on the use case ( I10). B.3 Project Planning Criteria\nProject planning criteria are set up to assess the considerations regarding fairness in the\nproject’s planning phase, where business understanding and process planning are done,\nand requirements for the project are set.', 'Interviewees are divided on whether project\nplanning criteria are useful to implement and what should be assessed with what kind\nof purpose in mind. In such early stages of the lifecycle, one interviewee argues it\nmight be challenging to set criteria for certifying fairness ( I13). Another interviewee\nargues that a consequence of criteria for business understanding may be neglecting\nsome products’ specialized nature, leaving little room for steering decisions ( I6).', 'Define & Assess the Planned Application Interviewee 14 argues that in a\ncertification process, the purpose of use and not the technology itself should be the focus\nof an assessment for certification ( I14). Hence, the problem definition that the system\nbuilds on should be investigated ( I8,I12,I13). That motivates an assessment that can\nalready occur before the actual implementation starts, even though it may be associated\nwith many manual processes ( I11). According to many interviewees, solid definitions of\nthe use case and the application planned are essential factors that should be checked\nin a fairness assessment ( I4,I8,I9,I12,I13,I14).', 'Moreover, users or stakeholders, in\ngeneral, need to be identified ( I8). In pursuit of being able to assess the impact on\nstakeholders and the planned application, three major concepts need to be considered:\nFirst, a good user and stakeholder understanding needs to be established. Second, the\nfairness properties of the use case need to be identified and assessed. Third, the planned\nsolution must be checked to determine whether its fit for the use case favors fairness. The hierarchical mapping of all relevant concepts can be found in Figure 6.', 'User & Stakeholder Understanding : User and stakeholder understanding capture\nassessing processes to gain knowledge about general attributes and characteristics of'])

(30, ['30 Freiberger et al. Fig. 6. Criteria relevant to “Define & Assess Application” hierarchically mapped\nstakeholders like demographics ( I6), a target group definition ( I8), the understanding of\npossibly multiple levels of distinct users involved ( I8), and the context users find them-\nselves in ( I11). The assessment of the context of the users is highlighted by interviewee\neleven as some stakeholders may be dependent on the system without having alter-\nnatives ( I11), making a fairness issue more impactful.', 'Interviewee eight highlights the\nimportance of an assessment for direct and indirect users as well as other stakeholders\nseparately, instead of a singular focus on the end user, to avoid vague results ( I8). This\nconception is supported by the fact that some systems operate in multi-sided markets,\nand fairness impacts multiple stakeholders ( I7). The composition of the target group\nshould be explained for an assessment to check ( I7,I8). Some use cases may natu-\nrally exclude certain groups without being unfair ( I7).', 'Moreover, the audited company\nshould have mechanisms to assess the likelihood of marginalized groups within the in-\ndividuals affected by the application ( I8). It should be assessed whether the planned\nsystem considers the particular interests of all its user groups ( I11). However, it needs\nto be checked whether such processes may lead the company to develop misconceptions\nof their stakeholders as there is a risk of misinterpretations about their true nature ( I9).', 'Use Case: Fairness Properties : Assessing the fairness properties of the use case\ninvolves checking the definition of fairness for the use case and the implications of\nthe latter on the defined stakeholders ( I12,I14). A definition can be seen as essential\nas fairness aspects can differ drastically between use cases and conceptualizations of\nfairness (as discussed in the previous chapter). It sets the focus of what needs to be\nconsidered for the technical system ( I14). As different stakeholders are involved, there\nmay be multiple fairness definitions to consider ( I12).', 'What can also be assessed in an\naudit is whether the fairness approach that the audited company chooses is suitable\nfor the use case ( I14). The underlying business case should also be assessed regarding\nfairness ( I9,I10,I14). An unfair underlying business case invalidates all attempts to\ncreate a fair system ( I9,I10). Interviewee 10 suggests utilizing a question catalog with\nyes/no questions on the use case. The business responsible for the project may fill it\nout and provide it for an audit to identify the fairness of a use case ( I10).', 'Another\nproperty of a use case that needs to be considered is its vulnerability to fairness risks\nwhich may be introduced via sensitive criteria ( I7). Name ( I5) and region ( I2,I5) are\njust two examples of such attributes. Credit scoring is mentioned as an exemplary'])

(31, ['Fairness Certification 31\napplication strongly affected by fairness risks ( I13). An assessment should not just\ncheck whether the fairness risks of a use case are being identified but also if there are\nmitigation strategies for those risks in place ( I7). The last property to consider is how\nmuch fairness is a viable business factor for the use case ( I13). It may indicate to the\nauditor how much the company is by itself incentivized to take action. Planned Solution: Use Case Fit : The planned solution is investigated for its fit to\nthe use case.', 'That involves the ML model itself but also its application. As a first\nstep, it should be assessed if there is a viable alternative to the ML model in the form\nof a deterministic solution or a rule-based set that should be used instead ( I5,I12). Such solutions tend to be less affected by biases ( I4,I5). It can also be the case that a\nmanual process should be preferred based, for instance, on ethical considerations ( I12).', 'If an ML-based solution is chosen, model size and the amount of data flowing into a\nmodel should also be assessed ( I5). The bigger a model gets, the larger is the potential\nand complexity regarding biases ( I3). Smaller models usually also offer the advantage of\nbetter explainability ( I4), which is a topic discussed with the concept “Model Reporting\nand Transparency” in the chapter “Criteria for Governance”. However, there is typically\na trade-off in other KPIs like performance ( I4,I5). Moreover, the chosen objective\nshould be assessed for certification.', 'It should not represent a loosely connected and\njust correlated proxy for the business problem at hand but rather represent it most\naccurately ( I12). Often, correlated information on ethnicity is used for predicting an\nobjectively unrelated variable leading to harmful results, for instance, for the black\ncommunity ( I12). To check for that issue, the company should state its business goal so\nthe certifying institution can assess the chosen objective’s suitability ( I12). A mismatch\nshould be easily detectable by an auditor ( I12).', 'Fairness Targets in Requirements Another finding is that fairness should be\nembedded in the requirements set for the NLP project ( I2,I4,I9). Fairness-related\ncriteria should be kept in an abstract form to be applicable to all projects ( I2). When\ndefining tasks, these requirements should help to give direction in the form of a checklist\nof where to focus and where problems may occur, hence introducing a reflection process\n(I2). A requirements catalog should direct the development and improvement of the\nsystem towards systematically including fairness considerations by enforcing specific\nsteps for fairness to be considered ( I9).', 'It embodies the company’s vision regarding\nfairness and its commitment to it ( I2,I9). Such a requirements catalog or checklist\nmay be assessed in an audit ( I9). B.4 Data-related Criteria\nMost interviewees agree on the relevance of data-related criteria for certification of\nfairness ( I1,I4,I5,I6,I8,I9,I10,I13,I14). One general aspect that should be considered\nregarding data is, that all processes leading to the data that the model eventually uses,\nshould be reproducible for an audit ( I4).', 'Apart from the data assessment itself, it is also\ndiscussed how data should be managed and how it should be handled and transformed\nin processes over the lifecycle like annotation, preprocessing, evaluation, and continuous\nimprovement. Data Assessment Data assessment focuses on judging the suitability of data for\ntraining and retraining models that operate fairly. The data set on which it is trained'])

(32, ['32 Freiberger et al. Fig. 7. Criteria relevant to “Data Assessment” hierarchically mapped\ncan be seen as the major fairness issue ( I1,I2,I5,I6,I7,I10), which gives reason for\na thorough investigation. It needs to be assessed regarding its fairness, specifically\nregarding the use case it is intended to be used for ( I1). The data’s specific impact\non the overall system needs to be considered ( I5,I7). Subject matter experts may\nbe required to assess data regarding fairness in an audit ( I12).', 'However, it needs to\nbe considered that data may be scarce for specific use cases, leaving little room for\nimprovements for the audited company ( I6). As another limitation, interviewee five\nmentions that some information may also not be available for assessment in certain\ncases ( I5). Data assessment entails an assessment of the data quality by the auditor\nbut also an assessment of company processes that aim at avoiding data affecting fairness\nnegatively. All relevant concepts for data assessment are portrayed in Figure 7.', 'Data Quality Assessment : Data quality is a concept that covers to what extent\ndata captures reality completely and in an undistorted way while keeping suitability\nfor the use case in mind [79]. Other facets of data quality are not discussed here. Rep-\nresentativeness ( I1,I2,I4,I6,I7,I8,I9,I10,I11,I12,I13) and identifying harmful and\nincorrect data ( I2,I4,I8,I9,I10,I13,I14) are mentioned for the data quality assess-\nment. Representativeness captures data, including all the variables needed, to represent\nthe diversity of users of the system ( I12).', 'Interviewee twelve mentions the prominence\nof lacking representativeness of certain ethnic groups, causing fairness issues for ML\nmodels in the past ( I12). To assess the representativeness of data, it makes sense to\nintroduce distribution checks ( I1,I4,I8,I9,I13). Distributions can be aimed at being\nrepresentative of overall society ( I10,I12) or the system’s users ( I2,I6,I7,I11). Lacking\nrepresentation in distributions can either happen by the exclusion of certain groups ( I6,\nI7) or by skewed representation ( I1,I4,I6,I13) and should be explained ( I6). Repre-\nsentativeness is assessed for different attributes or target variables represented in data\ndistributions ( I4,I13).', 'For assessment, statistic screening ( I12) and frequency-based\nscreening ( I4,I10) may be utilized. The latter could also be described as a NER-based\nrepresentations screening ( I10). Words indicative of a class for a sensitive attribute\nand related words are automatically tracked via their semantic vector representation,\nwhich mirrors what NER does ( I10). Their frequencies are compared to the repre-\nsentative real-world ratio. A comparison to a reference dataset regarding frequencies'])

(33, ['Fairness Certification 33\nand semantic representation may also be utilized ( I10). Checking label distributions in\nthe annotation process may uncover some underlying data issues ( I6). One particular\nfactor that may impact representativeness after data acquisition is preprocessing. Rep-\nresentativeness may be negatively affected by data cleaning in the form of the removal\nof outliers ( I10,I12) or by sampling ( I4). That makes assessing distributions before\nand after preprocessing necessary ( I12). The sourcing and collection of data need some\nassessment by the certifying institution regarding representation issues.', 'It should be\nchecked whether the data distribution represents the use case ( I2,I6,I8). As data is\nsometimes procured from abroad and collected there in an entirely different societal\nbackground, there may be issues with its representativeness for the use case ( I6,I8). It could be checked in an assessment whether the application context of data meets\nits distributions. Similar issues occur when poorly fitting open datasets are used for\nfirst model iterations ( I2). Induced selection biases can be found by inspecting the\nunderlying data collection process ( I6,I12).', 'The context of time ( I12), process char-\nacteristics, education of subjects, and location of the collection could be considered\nand questioned regarding possible adverse downstream effects ( I6). Data augmentation\nalso needs to be mentioned, considering the representativeness of data ( I11,I13). It can\nbe assessed whether the baseline set for augmentation captures diverse enough data\nto represent the system’s users ( I11). Because then there likely will not be a negative\nimpact on representativeness after augmentation ( I11). A company could even utilize\ndata augmentation to improve the representativeness of data ( I13).', 'That may be done\nby augmenting larger quantities for the underrepresented group. However, this comes\nwith the limitation of a loss in label accuracy ( I13). The approach to data augmenta-\ntion and its downstream impacts may be relevant to check in an audit. Online learning\nutilization may enable a system to adjust automatically toward better representation\n(I9). It captures systems that automatically go through continuous improvement cycles\nby automated capturing of data for retraining ( I9). When representation issues can be\nidentified, the system could be set up to counter-steer automatically ( I9).', 'Harmful or\nincorrect data negatively impacts data quality, and systems should be checked for it\n(I2,I4,I10,I13,I14). Interviewee 13 mentions harmful contents in baseline data as an\nissue that should be checked, particularly for generative models ( I13). Misinformation\nin data is also an issue that should be investigated ( I8,I13). However, in some cases,\nthere is a grey area where it is difficult to judge to which extent data accurately rep-\nresents the true nature of facts ( I8).', 'The last factor that should be assessed regarding\nharmful data is word embeddings which become relevant in transfer learning ( I2,I4,\nI9,I14). The semantic representation of words in vector space needs to be checked for\nunwanted associations like gender with occupation ( I2,I4,I14). Therefore, distances\nbetween embeddings in vector space should be checked before training a specific model\n(I4,I14). A reference data set may again be established to set an expectation for vec-\ntor space representation ( I10). Embeddings do have a downstream impact.', 'Companies\ncan handle the impacts of vector space representation by utilizing readjustable models\n(I4) or in their decision-making by compensating for their impact ( I14). Such coping\nmechanisms may also be an aspect to assess for certification. Internal Processes Assessment : Apart from investigating the data itself, a com-\npany’s processes for sourcing, collecting, and checking it for biases should be assessed. Such mechanisms may be essential to consider as they can indicate how much stability\nof fairness in data over time can be expected.', 'The data sourcing assessment investigates\nhow the company procures its data and what considerations are made to ensure sourc-\ning quality data for building a fair system ( I13). The selection process for sources is a\ncentral consideration here ( I1,I2,I4,I8,I12,I13). There may, for instance, be quality'])

(34, ['34 Freiberger et al. indicators like reviews or reputation of a data set of interest ( I1), ratings for articles\nor scientific journals utilized ( I13), or, in the case of social media data, for instance,\nan up-vote to down-vote ratio ( I4). Social media, however, needs careful consideration\nregarding the intended use case as it tends to be very opinionated and often biased\n(I4). Sources renowned for extreme perspectives, which tend to be biased, should be\navoided ( I4,I8). When sourcing externally, it should be checked for transparency about\nthe data collection process ( I1,I12).', 'Sources used should be documented ( I4), and the\nsourced data should be reviewed regarding use case fit ( I2,I4,I5,I6,I13). The data\ncollection assessment checks for suitable processes to collect data ( I12,I14). The col-\nlection of data by the company itself, in contrast to data sourcing, gives the company\ncomplete control over its data acquisition ( I12). Guidelines for the use case should be\nset for the collection process to avoid bias and may be reviewed for certification ( I3).', 'Data is sometimes synthetic and, in the case of chatbots, sometimes a result of brain-\nstorming different ways of expression for responses in the development team ( I2). That\nis why special guidelines may be required to avoid inducing bias ( I2). Assessing forms\nof bias checks a company performs may help assess processes enforcing fairness ( I1,I6)\nat the cost of being resource intensive ( I2). Human involvement should be checked, as\nit makes sense to pursue a semi-automated approach in this case ( I2).', 'That is because\na human has a more refined sense of detecting relevant biases or fairness issues than a\nmachine ( I2). Hence, it should be checked whether a human assesses a data sample for\npotential fairness issues ( I2,I10). The company should establish processes for an explo-\nrative data understanding of the dataset, helping to assess its quality and contents ( I1,\nI4,I5,I10,I11). The contents, like ways of expression ( I5,I9), contained entities ( I5),\nand personalized data ( I5), already indicate potential biases in data towards which a\nsystem needs to be robust.', 'Even just the language that is used may indicate whether\na system is susceptible to gender bias ( I13). Moreover, data contents should also be\ninternally checked for diversity and inclusiveness ( I4,I7,I9,I11), potential stereotypes\nrepresented, and harmful content ( I1,I8). A company should investigate all of this,\nconsidering metadata describing data sources to get the context that may be relevant\nfor identifying issues ( I11). Data bias checks should also help the company uncover un-\nfairly annotated data ( I6,I12). That can be done by comparing incorrectly labeled data\nbetween groups upon review ( I12).', 'If such unfairness is discovered, a company should\ninvestigate why it happens ( I6). In some cases, a company may uncover bias in data,\nbut there may be no suitable mitigation strategies. That should lead to documentation\nof the issue for mitigation in downstream processes ( I3). Annotation The annotation process of data can be considered an important factor\nfor the fairness of a system based on supervised learning ( I4,I6).', 'When investigating\nthe annotation process of a company in an audit, it may be challenging to assess the\nquality of annotations in some cases as there may not be a ground truth that can be\nutilized for evaluation ( I12). Moreover, there is a high degree of subjectivity to annota-\ntion ( I5). If there is an objective ground truth, however, one may assume that a correct\nlabel is a fair label and evaluate fairness by the correctness of labels ( I12). To certify\nannotation, one may consider processes established in a company for annotating data.', 'Three major concepts are identified for company processes. First, fairness-relevant as-\npects in the data should be annotated in the data ( I1,I2,I11). That is a crucial step\nfor being able to evaluate the fairness of the system for an audit or continuous im-\nprovement ( I1,I2). Another way this benefits the development of fair systems is that\nit facilitates purposeful sampling allowing for fair representation of certain groups in'])

(35, ['Fairness Certification 35\ndata ( I2). The annotation process of fairness-relevant aspects may either be manually\nlabeled or automatically tagged ( I2). However, one drawback to such annotations is\nalso mentioned with its possible negative performance impact ( I1). Second, there is a\ntradeoff between labor cost and time and the quality of annotations to handle ( I2,I11). To a certain extent, saving cost and time will sacrifice quality and hence fairness of an-\nnotations. Interviewee four proposes an approach that may be requested of companies\nas an option to pursue a more reflected annotation strategy regarding label quality.', 'This tradeoff can be managed appropriately by including filters defining what needs to\nbe annotated manually ( I4). Third, there should be a set of measures implemented by\nthe company that causes a minimization of fairness risks in the annotation process ( I4,\nI5,I7,I10,I12,I13). Like every human, human annotators have biases that motivate\nannotation processes so that their biases are at least less reflected in their annotations\nof data ( I7,I11). While annotating data, one may require the annotator to see similar\nannotated examples to the respective one that should be annotated ( I7), facilitating a\nmore objective decision.', 'One may also require the process to be designed so that sensi-\ntive information stays hidden and cannot bias the annotators’ decisions ( I7). An aspect\nfrequently mentioned is involving multiple annotators in labeling a data point ( I4,I5,\nI7,I10,I12,I13). Interviewee four suggests that it should be case-dependent to either re-\nquire multiple annotators or not ( I4). Particularly for topics that are very opinionated\nand perceived differently by every individual, like politics, multiple annotators should\nbe involved to reduce biases ( I4).', 'The aggregation of multiple annotators’ opinions on\nthe correct label can either be achieved by a majority vote between annotators ( I5,I12)\nor by an Inter-Annotator Agreement ( I4,I7,I10,I13). An Inter-Annotator Agreement\nscore quantifies how similarly multiple annotators annotate the same pieces of text ( I4,\nI10). Particularly referring to topics that tend to be opinionated, a very high score rep-\nresents a high agreement which may indicate that annotators are in a bubble ( I10). It\nshould raise organizational questions which are discussed in “Criteria for Governance”.', 'However, in the case of developing generative models, finding a suitable metric as a\nbaseline for such an Inter-Annotator Agreement may be difficult ( I5). Another integral\napproach the audited company should pursue is utilizing annotation guidelines ( I4,I6,\nI7). These guidelines should be documented so they can be audited and as objective\nas possible ( I4). A beneficial consequence of introducing annotation guidelines besides\nfairness is more consistency and hence higher quality of annotation ( I6). Preprocessing Preprocessing includes all steps for transforming, cleaning, selecting,\nand expanding data after annotation for training and evaluation.', 'When auditing this\nstep in the lifecycle, it must be considered that its scope may vary strongly between\nuse cases ( I3,I11). That makes it problematic to require certain steps for all use cases. Instead, the steps taken should be documented and assessed regarding fairness impact\n(I4). One concept mentioned several times is that the steps in preprocessing should\nbe checked to determine whether they induce bias or counteract measures to improve\nfairness from previous lifecycle steps ( I10,I11,I12). The same criteria established to\nbe met by the data before preprocessing should also be met after preprocessing ( I12).', 'Robustness-ensuring mappings are a way to transform data by mapping it to a stan-\ndard form ( I2). They may be helpful to pursue in some use cases as they can remove\nusability hurdles ( I2). They can be, for instance, utilized to enable people with migra-\ntion backgrounds to achieve a similar performance compared to native speakers of a\nlanguage. For instance, articles can be mapped to a generic domain, making typical\nmistakes irrelevant ( I2). Anonymization may be a very related approach because it'])

(36, ['36 Freiberger et al. aims to drop the sensitive attribute in the data ( I3,I5,I7,I14). The issue with such an\nanonymization approach is that dropping the sensitive attribute makes it challenging\nto assess the fairness of a model at a later stage, as the information from the sensitive\nattribute is lost ( I7). Because of that, anonymization counteracts the annotation of\nfairness-relevant attributes as an earlier step in the lifecycle.', 'Another problem with\nanonymization is that it does not necessarily solve the problem of biased data, as there\nusually are indirect encodings through correlations that can materialize as second-order\neffects ( I8). In some use cases, anonymization may not be suited as it may undermine\nthe solution’s functionality because of a negative performance impact ( I5,I10). The\nsame applies to differential privacy, which is an approach for comparing the effect of\nincluding the sensitive attribute and dropping it ( I5,I7). Nevertheless, in some cases,\nit may be viable and should be considered by the company ( I5,I7).', 'An important\nconsideration for preprocessing that may be assessed is the filtering and selection of\ndata ( I3,I4,I13). Filtering and selection aim to improve the data quality for use via\nfiltering mechanisms and ensure appropriate data splitting. Filtering may target re-\nmoving harmful content like obscene language ( I13) or low-quality or biased data ( I3,\nI4). However, it may be challenging for some topics to identify biased data clearly ( I13). Moreover, defining fair filters objectively and calibrating how restrictive they should\nbe poses a challenge ( I4).', 'Particularly the use of social media data raises the question\nof controllability for such a process ( I13). When selecting data, unbiased and balanced\ndata sampling is essential ( I1,I4,I7,I8,I10,I12,I13). Depending on how data is selected,\na biased selection could be made, which is to be avoided ( I1,I7,I10,I12). When split-\nting the data into training data and data for evaluation, it should be assessed whether\nthe distributions of training, evaluation, and overall data roughly match ( I10,I13). As\nmentioned, that helps avoid bias and ensures the integrity of evaluation results ( I10).', 'It\nwas also mentioned that the data splitting might consider underrepresented minorities\nby representing them in the data samples equally compared to other instances that are\nbetter represented ( I10). Data for Evaluation In evaluation, some aspects and concepts regarding data\nmust be considered. Functional testing of a system is input-based testing assessing the\noutputs and hence views the system from an external perspective suited for an audit\n(I10). The procedure is discussed in the chapter “Modeling & Evaluation Criteria”. Test data is targeted at unveiling biases present in the model ( I3).', 'It should represent\nthe diversity of actual usage of the system and the challenges that come with it ( I2,\nI11). Moreover, it should suit the assessment regarding the specific fairness definition\nimposed by the audit ( I9). A relevant consideration is the sourcing of data sets utilized\nfor evaluation. It could be acquired by accumulating in-use data of the system over time\n(I12). Another way of acquiring test data is going by previous precedents of fairness\nissues and utilizing relevant data regarding these ( I13).', 'The last approach to getting\ndata is artificially generating it, making a targeted evaluation of fairness easier as it\ncan be designed to reveal biases ( I2,I6). Targeted segmentation for sampling data of\ninputs can help assess specific biases ( I7). However, these data sets may also be biased\nand only provide partial insight into fairness issues ( I8). Another vital consideration\nfor functional testing is utilizing fairness invariance tests ( I2,I3). Such tests utilize\ntargeted replacements in inputs to assess the model’s invariance to variation in sensitive\nattributes’ values ( I2).', 'These replacements, as a form of augmentation, are usually\ngenerated by multiple language models ( I2) and involve approaches like forward and\nbackward translation ( I2,I3). Drawbacks to this approach are mentioned with the'])

(37, ['Fairness Certification 37\ncomplexity of such augmentations in NLP ( I3) and potential model biases present in\nthe models utilized for generating replacements ( I3). Nonetheless, interviewee two sees\nrobustness invariance tests as an integral part of certifying the fairness of a system ( I2). It is suggested to propose a range of measures and a minimum standard for creating\nsuch augmentations for fairness invariance tests ( I2). Data for Continuous Improvement Data-related criteria in continuous im-\nprovement target data considerations for maintaining a fair system over the time of\nits operation.', 'That involves considering monitoring practices as well as feedback loop\npractices. Continuous improvement is affected regarding data by two challenges: main-\ntaining data diversity ( I9) and extending continuously relevant testing data ( I9). As\ncriteria for monitoring, test sets ( I2,I3,I6,I8,I9), drift monitoring ( I2,I3,I4,I5,I6,I7,\nI13), as well as a request assessment for underrepresented groups ( I2) are mentioned as\npractices to be implemented by the audited company.', 'Test sets in continuous improve-\nment are the same data-wise compared to test sets in evaluation, but they are used\ncontinuously to assess whether fairness can be maintained or whether certain issues\nhave occurred ( I2,I3,I6,I8,I9). Drift monitoring is described either as data-centric\n(I2,I4,I5,I7,I13) or as model-centric ( I2,I4,I5) in its approach. It may reveal the\nnecessity to retrain a system to stay fair and performant over time ( I2,I3,I4,I5,I6,\nI13).', 'A model-centric approach considers model behavior like changing performance or\nconfidence in predictions ( I2,I5) and explainability ( I2,I4) of the latter, in the form\nof explanation scores or some other form of quality assessment for the explanation,\nin detecting drifts. In a data-centric approach, the variation of incoming data for the\nmodel to handle compared to the original training data is assessed ( I2,I4,I5,I7,I13). Continuously appearing outliers may be an indication ( I2). It may be helpful to trace\nback to the kind of requests causing the drift to understand the underlying issue ( I2).', 'The reaction strategies to an occurring drift may be interesting for an audit ( I3). When\nusers change, assessing them for semantic overrepresentation, which may cause an un-\nfavorable data drift, may also be relevant ( I2). The frequency at which such drifts may\noccur is dependent on the dynamics of the environment in which the application is\noperational ( I2,I5). That should impact the assessment of the calibration of such a\ndrift monitoring system. An assessment should also consider that drift detection may\nlack a measurable foundation for some use cases ( I3).', 'Hence, it should only be required\nfor changing environments and if it is measurable. The consequences of drift monitor-\ning may materialize in improved fairness and performance, making it compatible with\nbusiness goals ( I2,I3). An assessment of requests for underrepresented groups focuses\non better understanding marginalized groups’ requests ( I2). That helps uncover po-\ntential issues to counteract the negative impact of underrepresentation in data they\nface ( I2). That may be performed by checking semantic differences in the expression\nof marginalized groups compared to better-represented groups ( I2).', 'Substantial devi-\nations may indicate potential issues in system performance later ( I2). Feedback loop\npractices regarding data need to consider how in-use data and user-made corrections\nare handled. One condition that needs to be met by both is user privacy ( I7,I9,I10). That may target a confidential treatment of user feedback ( I7) as well as removing\nsensitive attributes from that feedback ( I10) and ensuring data protection and data\nsecurity when saving data ( I9).', 'In-use data is gathered by regular operation of the sys-\ntem without any user interaction targeted directly at generating feedback and should\nalso be subject to assessment ( I5,I9). One form of in-use data that can be utilized is\nprediction queries dependent on confidence in the prediction, either taken for retrain-'])

(38, ['38 Freiberger et al. ing future models in case of low confidence or for usage as test data otherwise ( I9). That works in favor of underrepresented groups as their queries are more likely to be\npredicted with low confidence and hence taken for model improvement. Metadata may\nbe essential to understand fairness-relevant aspects in such a case ( I6,I9). However, if\nmetadata is unavailable, the context of the query stays hidden, and it may be challeng-\ning to target model improvement toward fairness ( I9).', 'Another form of in-use data can\nbe gathered by indirectly measuring user discontent by analyzing behavior when the\nuser interacts with the application ( I5). That may indicate a system’s impact on its\nusers, and after passing a certain threshold, it can be estimated to be unfair, requiring\nintervention ( I5). The indirect form of measurement offers the advantage of gather-\ning data without influencing or biasing the user in their feedback statement, which\nmay happen when directly asking for it ( I5). A dedicated team or a quality assessor\nshould check usage data regularly for such aspects ( I5).', 'If users can make immediate\ncorrections or give feedback which may be taken to improve the system, particularly\nthe quality of feedback data needs attention as feedback may be biased or incorrect\n(I4). If the user is facilitated to correct an annotation, clear guidelines should be estab-\nlished by the company and communicated to the user ( I4). Hence, data is consistent\nwith the originally annotated data, and incorrect annotations may be prevented. The\nlatter should also be enforced by a controlling instance that has to review and accept\nuser annotations ( I4).', 'Moreover, there should be some form of filter rejecting harmful,\nlike for instance stereotyped, feedback data generated by the user from being utilized\nfor model retraining ( I10). There are industry examples where such data has caused a\nsystem to become heavily biased ( I9,I10). Data Storage Regarding data storage which covers how data is stored in develop-\nment and operations and cached in operations, two concepts are introduced. First, the\nstorage of sensitive attributes should be avoided in a raw form ( I10).', 'That has gained\nrelevance due to the trend towards the ELT (Extract Load Transform) paradigm, as\nthere is an interest in utilizing the raw and not the transformed data ( I10). Avoid-\ning saving sensitive attributes raw can also be partially motivated by compliance with\nGDPR, as personally identifiable information should not be saved ( I10). This focus on\nprivacy highlights again that fairness is interrelated with other RAI topics. Second,\ninterviewee one mentions caching of previous predictions of the system as a potential\nissue ( I1).', 'In case of unfair previous predictions, this necessitates clearing the cash to\nrestore fairness of the system ( I1)). B.5 Modeling & Evaluation Criteria\nIn this chapter, fairness criteria for building NLP models and evaluating them and the\nsystem they are embedded in are explained. Modeling Modeling, particularly model selection, is perceived as less critical for\ncertifying fairness compared to criteria targeting other lifecycle steps ( I1,I5,I10). A\npotential issue mentioned regarding the utilization of pretrained models is a lacking\nholistic understanding of it ( I3), making it hard to assess and enforce fairness for train-\ning the custom model.', 'That leads interviewee three to suggest only permitting the\nusage of, to some degree, trustworthy and documented baseline models for certification\n(I3). That could solve the existing business process and documentation conflict in case'])

(39, ['Fairness Certification 39\na poorly documented model outperforms a well-documented one ( I3). This would also\nincentivize providers of performant models to document them appropriately. When de-\nveloping and training models, fairness can be rooted in the model architecture ( I2,I4,\nI5,I6,I7,I10,I14). For doing so, interviews reveal four potential strategies. The first\nintroduces a fairness classifier built onto the model used ( I5). For implementing some-\nthing like this, however, a labeling of fairness for all annotations used in the original\nmodel would be required, which would be very resource-consuming and impractical\n(I5).', 'The second approach embeds fairness into the objectives optimized by the model\n(I4,I5,I7). That may be done by performing a hyper-parameter optimization of subjec-\ntive fairness, even though it would be time and resource-consuming ( I5). Another way\nwould be to calculate the loss function, which is to be optimized only on non-sensitive\nattributes or to penalize using the latter ( I10). Either way, the question needs to be\nasked whether the intended model is suitable for such approaches in practice ( I7).', 'The\nthird approach aims at explicitly calibrating a model to avoid certain biases or, in a\nway, hard-code fairness into the system ( I4,I5,I6,I14). That may go beyond the NLP\nsystems prediction and involve rules for decision-making based on the prediction ( I6,\nI14). As a fourth approach, interviewee two suggests introducing constraints for model\ncontrollability ( I2). Language generation via a prompt-based model is mentioned as\nan example where there should be the possibility to avoid biases by setting certain\nparameters influencing constraints for the model ( I2).', 'Otherwise, the model would\nmirror potential biases in baseline data ( I2). All of the mentioned approaches should\nbe assessed regarding their impact on the performance and fairness of the system ( I5). Evaluation Evaluation is essential for assessing a model’s fairness ( I3,I7,I9). It\ninvolves testing the model or the system it is embedded in regarding various targets\nor metrics. Reproducibility must be given for company internal evaluation processes\nso they can be assessed in an audit ( I8). A challenging factor may be the definition of\nfairness which is the foundation for evaluating fairness ( I8,I12).', 'As the definition of\nfairness differs between the auditor and all affected stakeholders ( I12), a hierarchical\napproach to evaluation focused on the system’s stakeholders may make sense as a base-\nline for evaluation ( I8). Another challenging factor is to be found in the context of data\nas a prediction foundation. Sometimes truthfulness or accuracy may be questionable\nbut hard to verify, posing a major issue for evaluation ( I8). In open-domain systems like\nquestion-answering systems or recommender systems delivering multiple suggestions,\nthe complexity of human language becomes a challenge for fairness evaluation ( I2,I8).', 'The auditor may assess all criteria in this chapter directly as system tests. However,\nthey should also be embedded in the company’s internal evaluation processes, which\ncan be audited. Interviewees name two major concepts that drive evaluation. Func-\ntional testing criteria ( I1,I2,I3,I4,I5,I6,I7,I8,I9,I11,I12,I13,I14) and metrics ( I2,\nI4,I7,I8,I9,I10,I14) are defining factors identified for assessing fairness in evaluation. Functional Testing Criteria : Functional testing criteria take an outside perspective\non the system’s outputs given some specific inputs. An outside perspective on the appli-\ncation is taken, and no deep insights into the model architecture are utilized here ( I9).', 'The outputs for a given set of inputs indicate model behavior and resulting fairness\nissues ( I1,I5,I6,I7,I9,I11,I12,I14). Functional testing of the model can be considered\nan essential part of a certification process for fairness as it captures what the model\npredicts and what impacts its users ( I9). In comparison, the already mentioned data is\nthe model’s baseline, but with nonlinear relationships of a model, even seemingly fair\ndata can result in biased models ( I8,I9). Hence, functional testing criteria capture the'])

(40, ['40 Freiberger et al. actual fairness of a system better. For such an approach to be meaningful, a relevant\nscope of inputs is required regarding quantity ( I2) and diversity of input data, repre-\nsenting a wide variety of user groups ( I9,I11). That may be facilitated via a mainly\nautomated approach to testing ( I2,I3,I4,I11). Adversarial testing is mentioned for\nprompt-based generative language models ( I1,I4,I7). In that case, it involves utilizing\nspecific queries that suggest or invoke stereotypes is a practical approach for fairness\ntesting ( I1,I4,I7).', 'If the assumption of the stereotype is met by the most probable\nwords to be inferred by the generative model, a fairness issue is detected ( I1,I7). These\nassociations made by the model can be assessed in an audit ( I4). Interviewee seven also\npoints out the necessity of finding a metric for evaluating the fairness of a model going\noff a defined set of prompts ( I7). Another approach is utilizing specific test data sets\n(I2,I3,I6,I8,I9,I11,I13). The data utilized here has been discussed previously in\nthe chapter “Data-related Criteria”.', 'The mentioned data is used as model input with\na specific expectation for the predictions’ quality ( I2). It may make sense to analyze\nthe distribution of predictions to assess their quality, which may reveal systemic issues\n(I1,I11,I12). Interviewee one highlights that biased distributions in data should be\naccepted as a reality ( I1). However, a change in decision-relevant variables like qualifi-\ncations in a recruitment context should also change the distribution of predictions for\na specific group ( I1). Hence, relevant variables to the topic should be checked to be the\ndecision drivers.', 'The latter may be achieved utilizing explainability tools discussed in\n“Model reporting & Transparency” in the “Criteria for Governance” chapter. Decision-\nirrelevant variables should, however, not influence model inference ( I2). This can be\nchecked by introducing robustness invariance testing ( I1,I2,I4), which should be an\nessential part of a fairness audit ( I2). It aims to assess how capable a system is of gen-\neralizing regarding a sensitive attribute. The latter may be, for instance, represented\nin expression difficulties by non-native speakers of a language ( I2).', 'Protected groups\nmay be introduced, representing sensitive attributes that could become an issue in the\nuse case ( I2). By augmenting initial test data via replacements for protected groups or\nthe addition of nonsense, the robustness of the model can be checked in the form of\nadversarial testing ( I2,I4). For most use cases, a larger invariance between a protected\ngroup and the rest of the users results in improved fairness ( I2,I4). However, there are\nexceptions, like the medical domain, where separation is essential and should be the\nfocus ( I3,I13,I14).', 'Assessment should be performed automatically with the additional\nrequirement of involving a human assessment of a subset of data as a plausibility check\nfor the automated assessment ( I2). This invariance testing is enabled by annotating\nfairness-relevant aspects like gender in data ( I2). Predictions of the model may also be\nrequired to be validated ( I1,I4,I6). Simpler models and particularly rule-based sets\nhave the advantage of being typically less affected by biases, and decisions based on\nthem tend to be based on relevant factors ( I4).', 'Previously set expectations may also\nbe used to validate a model’s prediction ( I1,I6). A discrepancy between prediction and\nwhat is used for validation should be investigated ( I6). Interviewee five suggests af-\nfected stakeholders as a source of functional evaluation regarding the system’s fairness\n(I5). As a deciding factor to overcome individual differences in opinions on the fairness\nof the system, a majority vote or some other kind of threshold may be introduced ( I5).', 'The challenge of possible discrimination of minorities which such an approach could\ninduce, can be addressed by sampling minorities representatively when selecting the\ncomposition of affected individuals responsible for evaluation ( I5). An audit assessment\nshould also consider ethics and moral considerations relevant to the system ( I4,I11). As\nfairness and ethics are closely related, there needs to be a discussion as to what extent\nfairness should represent normative values in a society ( I9). A central question here is:'])

(41, ['Fairness Certification 41\nIs it fair if a system results in morally questionable actions that affect all groups in the\nsame way ( I9)? A good fairness understanding of the use case, which is discussed in\n“Process criteria” is required to resolve ambiguity. The last criteria to consider for func-\ntional testing focus on the interaction of humans with computers and the involvement\nof humans to improve fairness. Humans sometimes tend to accept predictions of ma-\nchines as accurate and tend to become complacent in reviewing system outputs before\nutilizing them for decision-making, which is called automation bias.', 'This automation\nbias is to be avoided as it does not counteract biases in predictions from impacting\ndecision-making based on them ( I11). Hence, there should also be criteria for human\ndecision-making that counteract automation bias ( I11). Humans should generally be\ninvolved and in the loop when assessing system fairness ( I2,I7,I11). Regarding the ap-\nplication of the model, there should be an assessment regarding the necessity of human\ninvolvement in decision-making. In specific, particularly critical applications, human\ninvolvement in decision-making may be crucial ( I12).', 'Many fairness issues are only ap-\nparent to humans and may currently not be automatically detectable ( I2). Moreover,\nhumans may give the automatic system a plausibility check ( I2). Metrics : Metrics aim to make the fairness of NLP models measurable. Metrics are\nof essential importance for a targeted, quantifiable and reproducible assessment of the\nfairness of an application ( I7,I8,I12). Metrics based on a generalization of the model\nare named as well as metrics that follow an impact-based approach regarding fairness.', 'Robustness and generalization may be seen as essential elements in assuring confidence\nin the model’s property of not generating unexpected, unfair results ( I2,I4,I8). For the\nfirst, the expected calibration error is named as a metric keeping track of the robustness\nof a model ( I8). Also, standard metrics commonly utilized in data science may already\nindicate the generalization of a model and hence may be assessed ( I4). The impact-\nbased approach targets potential fairness issues for particular groups more directly ( I8),\nwhich manifests in the composition of fairness metrics.', 'A major point of discussion in\nsuch metrics is their level of specificity ( I3,I7,I14). A specific metric for fairness will\nnot easily transfer between use cases as there is yet to be a dynamic fairness metric\ncapable of handling that variety ( I7). Less specificity in the metric will lead to less\nrelevant results in the specific use case ( I3,I14). Interviewee eight hints at a trade-off\nbetween performance and fairness metrics that may need to be made by sacrificing some\nperformance for the merit of the impact-based metric ( I8).', 'Also, some metrics should\nconsider and capture diversity information as a context when assessing performance\nmetrics ( I8). Another impact-based metric would be a performance disparity threshold\nthat needs to be met ( I2,I7,I9). The quality of service provided by the system should be\nevaluated regarding differences between groups, and a threshold should set a maximum\nacceptable deviation ( I7,I9). That may be expanded beyond the mere performance of\nthe system and might include factors like accessibility and usability of the system as\nwell ( I2,I9).', 'The latter aims at removing barriers to system use ( I2) and may be less\nquantifiable than just performance disparity. Another approach would be introducing\na metric that penalizes the model’s use of sensitive attributes or unethical content. It\nwould be used by the optimizer of the model when calculating loss ( I10). By this kind\nof reinforcement, some major fairness issues that could occur may be ruled out ( I10). For instance, in question answering, responses that contain insensitive or stereotyped\ncontent could be avoided ( I10).', 'A wide variety of fairness metrics are also represented\nin different definitions of fairness available to choose from ( I8,I12,I14). No matter what\nmetric is used in the end, its impact on the technical system must be assessed, and its\nsuitability and implications for the use case must be assessed ( I1,I8).'])

(42, ['42 Freiberger et al. B.6 Operations Criteria\nBringing a system into operation and going through continuous improvement cycles\nto maintain its quality should be investigated in a fairness certification process. These\nsteps in the lifecycle determine whether a system will maintain fairness over time ( I9). Fairness can deteriorate in operations, for instance, at the cost of performance ( I2). Interviewee six mentions that the certificate must maintain integrity over time ( I6),\nwhich supports the relevance of system operations criteria for fairness certification. Operations need to build quick reaction strategies to counteract fairness issues that\noccur swiftly ( I1,I7,I11).', 'Standard pipelines for retraining and publishing models are\nnot viable as they take too long ( I7). Another consideration in operations is the resource\ndistribution for models, which should not put fair models at a disadvantage regarding\nresource access compared to other models ( I10). When drastic changes are made or\noccur in a systems environment, there may be a requirement to renew the overall\ncertification assessment or at least those assessments affected by the change ( I5,I7). Such changes may come in various forms. A new version roll-out may be a reason ( I1,\nI7,I8).', 'There should also be detailed documentation throughout model versions to\nensure traceability when a fairness issue occurs ( I1). Adaptions in data ( I1,I7,I9),\nchanging stakeholders ( I7), or a change in the system’s application ( I8) may be further\nreasons for renewing affected assessment steps. Deployment ( I2,I3,I6), monitoring ( I1,\nI2,I3,I5,I7,I8,I9,I13), and the feedback loop for continuous improvement ( I1,I2,I3,\nI4,I6,I7,I8,I9,I10) are relevant to interviewees as the main criteria for operations,\nDeployment Criteria for deployment are mainly brought up in the context of pro-\nviding different model sizes via pruning or knowledge distillation approaches ( I2).', 'First,\nit needs to be checked whether pruned or quantized models still fulfill fairness-related\ncriteria established before to the same extent as their larger counterparts do ( I2). It\nmakes little sense to perform an intense assessment for the large model that the user\ndoes not utilize in practice and, for instance, miss out on functional assessment of the\nsmaller model that a user may use more frequently ( I2). Smaller models tend to be\nmore prone to underfitting issues and should be checked accordingly ( I3).', 'On the other\nhand, larger models may need more testing as they are opaque and tend to contain\nmore complex and challenging to identify biases with more data being used for training\nthem ( I3,I6). When choosing a suitable model size, performance and fairness objectives\nshould play a role ( I2). Fig. 8. Criteria relevant to “Monitoring” hierarchically mapped\nMonitoring Monitoring can be seen as a set of mechanisms that ensure fairness by\nonly giving clearance for a new model version to be introduced or current versions to'])

(43, ['Fairness Certification 43\nstay operational if fairness criteria can be assured ( I1,I5,I7,I9,I13). It typically involves\nfairness testing and intervention mechanisms, as shown in Figure 8. Fairness tests have\nalready been introduced under “Functional Testing Criteria” in the “Modeling & Eval-\nuation Criteria” chapter. Such an evaluation process based on fairness test sets should\nbe extended to assess the model over time as it goes through continuous improvement\ncycles ( I1,I2,I3,I7). The goal is to quantify the occurrence of fairness issues ( I3) which\nmay be valuable information for an audit.', 'With every new model or change in data or\nparameters, the fairness dimension of the system may be affected ( I2). Interviewee one\nmentions the importance of standards that can be systematically checked with every\nsystem retraining ( I1). Such testing could be run at a very granular level assessing,\nfor instance, confidence and explanation scores on a sentence level as a foundation for\nlater judging fairness ( I2). Passing scores or red flags should be established for fairness\ntest sets to make the fairness testing impactful ( I2,I3). Interviewees are divided on\nhow automated such testing can be.', 'While interviewees one and nine see potential in\nautomated fairness testing, interviewee seven argues the process is not automatable as\nthere is no dynamic fairness metric ( I1,I7,I9). Depending on the automation of the\ntesting, frequent testing is favorable ( I1,I3). In large-scale organizations or in a highly\ndynamic context, where retraining models takes place multiple times a day, a fairness\nassessment with every retraining could cause a considerable overhead ( I3). The com-\npany may execute this fairness testing process right after quality assessment to assess\nthe fairness impact of changes in quality ( I9).', 'In some cases, a system may be oper-\nated or operating in a potentially harmful way. To prevent the system from causing\ndamage, intervention mechanisms are established. Particularly for transfer learning,\nthere is the issue of a user or third party potentially misusing a system for unintended,\nunethical purposes like spreading misinformation, which needs to be avoided if possible\n(I13). Interviewee 13 mentions the curse of globalization in this context, as traceability\nof what is built with baseline models stays unknown to the provider ( I13).', 'Moreover,\nmisuse has previously also caused issues with systems with automatic retraining on\nusage data ( I9). Targeted, hostile user behavior in the form of interactions with the\nsystem can make it develop strikingly unfair tendencies ( I9,I10). Manual controls and\nautomated corrective mechanisms on a quantifiable foundation may be implemented\nto ensure system fairness over time ( I9). Generally, a certification process could check\nwhether the issue of misuse is addressed and whether potential countermeasures are\nin place.', 'Moderating users, on the other hand, can be an option to cope with user\nbehavior that negatively affects a system’s fairness ( I3). Interviewee three mentions\nfour shortcomings of moderating users of a system. Its practicality is questionable as\na large volume of data needs to be handled, making it impossible to have human over-\nsight ( I3). That leads to utilizing models for moderation which may also be biased ( I3). Moreover, moderation may just cause a problem shift by users utilizing competitors’\nservices, making moderation, not incentive compatible as it counteracts business goals\n(I3).', 'In case a system develops extreme, unacceptable behavior, there also needs to be\nsome form of kill switch, as running the system until the next update is rolled out may\nsometimes be unacceptable ( I7). Feedback Loop Typically, a system is not designed to be static but rather dynamic,\nutilizing feedback data from its use for improvements ( I9). That offers the advantages\nof outgrowing systemic biases ( I6) and raising general model quality ( I2), which aligns\nwith business goals.', 'A similar approach to active learning may be beneficial for creating\nan effective feedback loop for counteracting biases in the system ( I4). By deducing'])

(44, ['44 Freiberger et al. Fig. 9. Criteria relevant to the “Feedback Loop” hierarchically mapped\nsystematic characteristics of biased cases and giving them as input back to users, their\nfeedback can be more targeted ( I4). To operationalize a system’s feedback loop, an\ninterface for the user to provide feedback or potentially even corrections for fairness\nissues is required. The acquired feedback data needs to be used appropriately to improve\nthe system. The latter is displayed in Figure 9. Feedback Interface : The interface should allow users to interact with the system ( I1,\nI2,I3,I7).', 'A challenge such interfaces must overcome is engaging the user to utilize\nthem ( I2,I4). There may be a reluctance due to convenience ( I4) or disengagement\nand disinterest in continuing to use the system if someone is offended by an occurring\nfairness issue ( I2). An audit might consider the extent to which such interaction mech-\nanisms are utilized to evaluate their effectiveness in exposing fairness issues so they can\nbe addressed.', 'In some cases, it may also be challenging to establish an interface in the\nfirst place as the user only indirectly encounters the model a few stages downstream\nin the process of an application ( I3). So, an audit should consider how the model is\nembedded in a system. An organization may also have user-centric teams that extend\nfeedback information acquired in a visual interface integrated into the application ( I7).', 'Those user-centric teams must closely collaborate with the organization’s development\nteam as their input may be required to identify previously unknown fairness issues,\nwhich can then be replicated and mitigated by the development team ( I7). It may\nmake sense to check for an interface for users to correct inference of a system ( I4,I8). The number of corrections made by users may also be relevant information for a cer-\ntification ( I4). Interviewee 4 also mentions corrections that can be made for a model\nexplanation as feedback on what users would base the prediction on ( I4).', 'The second\nform of interface targets reporting or flagging fairness issues encountered in use ( I1,\nI2,I3,I7). The users should be able to express their discontent with the fairness issue\nthat they should be able to describe ( I1,I2,I3,I7). It should be checked whether such\nan interface is immediately accessible by a user ( I2). Use of Feedback Data : The issue with such interfaces is a tendency to shift ac-\ncountability for fairness onto the system user, who may also be biased ( I6).', 'Hence,\nfor certification, the use of feedback data and accountability that comes along with\nit should be checked. That covers correctly understanding the fairness issue at hand,\nevaluation and preprocessing of data that should flow into retraining a system, and\nmechanisms for deciding when to retrain the system. Understanding the fairness issue'])

(45, ['Fairness Certification 45\nat hand involves the challenge of tracing the fairness issue back to the part of the\nsystem where it originates ( I1,I8). Mechanisms for understanding the cause of the fair-\nness issue should be implemented ( I1,I2,I3,I7). Assessing the expectation of the user\nand his or her explanation of unfairness for the prediction may be a first step in un-\nderstanding the issue ( I1). Tracking who specifically reports fairness issues may hint,\nfor instance, at a marginalized group that may have been neglected in development\n(I2).', 'Depending on the system, such an investigation may be only done if a minimum\nnumber of reports mention the same fairness issue ( I3). After identifying the primary\ncause, the issue may be tested and reproduced as a baseline for mitigation ( I7). The\nincoming feedback data from model use that should go into retraining should not be\nused without evaluating incoming data and preprocessing ( I10). This step should be\nautomated ( I10) as human involvement would be impractical with the amount of data.', 'The evaluation of incoming data and preprocessing measures may be guided by what\nhas been previously performed to assess data quality and to preprocess data ( I10). For\ninstance, sensitive attributes may be removed as it may be done in the initial pre-\nprocessing ( I10). Retraining should be initiated in case a fairness issue is repeatedly\nreported ( I3) or regularly based on feedback data from model use ( I4). It should also be\nsystematically checked whether the issue is removed with retraining ( I1).', 'For utilizing\nfeedback data in the most effective way possible, an active learning approach may give\ndirections on what is needed for improvements regarding biases ( I4).'])

